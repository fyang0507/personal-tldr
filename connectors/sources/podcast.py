"""
[GENERATED BY CURSOR]
Podcast Connector for fetching content from podcast channels.

This module provides functionality to:
1. Check for updates (new episodes) from podcast channels
2. Fetch detailed metadata for specific podcast episodes

The connector uses the Apple Podcasts API for searching and RSS feeds for content retrieval.
Ref: https://developer.apple.com/library/archive/documentation/AudioVideo/Conceptual/iTuneSearchAPI/index.html
"""
import requests
from utils.logging_config import logger
from utils.connector_cache import ConnectorCache
from typing import Optional, Dict, Any
import asyncio
from connectors.sources.base_source import SourceConnector
import json
import re
from services.llm import api_text_completion
from utils.toml_loader import load_toml_file
import os

class PodcastConnector(SourceConnector):
    """
    Connector for fetching content from podcast channels.
    """
    def __init__(self, podcast_name: str, debug: bool = False):
        super().__init__(source_identifier=podcast_name)
        self.podcast_name = podcast_name
        self.cache = ConnectorCache()
        self.debug = debug


    def _get_podcast_feed_url(self) -> Optional[str]:
        """
        Get the RSS feed URL for a podcast by name using iTunes Search API.
        """
        base_url = "https://itunes.apple.com/search"
        
        # Check if the podcast name contains Chinese characters
        # If so, use the CN country code, otherwise use US
        country = "CN" if any('\u4e00' <= char <= '\u9fff' for char in self.podcast_name) else "US"
        logger.debug(f"Using country={country} for podcast: {self.podcast_name}")
        
        params = {
            "term": self.podcast_name, 
            "entity": "podcast", 
            "limit": 1,
            "country": country,
        }

        try:
            response = requests.get(base_url, params=params)
            response.raise_for_status() # Raise an exception for HTTP errors
            results = response.json().get('results')
            return results[0]['feedUrl']
        except requests.RequestException as e:
            logger.error(f"Error fetching podcast feed URL for {self.podcast_name}: {e}")
            return None
        except (KeyError, IndexError) as e:
            logger.error(f"Error parsing iTunes API response for {self.podcast_name}: {e}")
            return None 


    def extract_first_item(self, xml_text: str) -> str:
        """
        Extract only the channel metadata and first <item> element from a podcast RSS feed,
        preserving namespace declarations.
        
        Args:
            xml_text: Complete XML text
            
        Returns:
            Trimmed XML with only the first item, preserving namespaces
        """
        try:
            # Extract the XML declaration if present
            xml_declaration = ""
            xml_decl_match = re.match(r'<\?xml.*?\?>', xml_text, re.DOTALL)
            if xml_decl_match:
                xml_declaration = xml_decl_match.group(0)
            
            # Extract the root element opening tag with all namespace declarations
            root_tag_match = re.search(r'<rss[^>]*>', xml_text, re.DOTALL)
            if not root_tag_match:
                logger.error("Could not find rss root tag")
                return xml_text[:10000]  # Fallback to simple truncation
                
            root_opening_tag = root_tag_match.group(0)
            
            # Find the channel element and its content
            channel_pattern = r'<channel>(.*?)<item>(.*?)</item>'
            channel_match = re.search(channel_pattern, xml_text, re.DOTALL)
            
            if not channel_match:
                logger.error("Could not find channel or first item")
                return xml_text[:10000]  # Fallback to simple truncation
                
            channel_content = channel_match.group(1)
            first_item = channel_match.group(2)
            
            # Assemble the trimmed XML with all necessary parts
            trimmed_xml = (
                f"{xml_declaration}\n"
                f"{root_opening_tag}\n"
                f"<channel>{channel_content}"
                f"<item>{first_item}</item>\n"
                f"</channel>\n"
                f"</rss>"
            )
            logger.success("Successfully trimmed podcast feed to first item")
            return trimmed_xml
        
        except Exception as e:
            logger.error(f"Error in improved_extract_first_item: {e}")
            # Fallback to returning a limited portion
            return xml_text[:10000]

    async def _process_podcast_feed_llm(self, feed_url: str) -> Optional[Dict[str, Any]]:
        """
        Process a podcast RSS feed using an LLM to extract episode details.
        """
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                "Accept-Language": "en-US,en;q=0.9",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Cache-Control": "max-age=0"
            }
            response = await asyncio.to_thread(requests.get, feed_url, timeout=20, headers=headers)
            response.raise_for_status()
            xml_content_original = response.text

            if self.debug:
                original_file_path = os.path.join("data", f"{self.podcast_name}_original.xml")
                with open(original_file_path, "w", encoding="utf-8") as f:
                    f.write(xml_content_original)
                logger.debug(f"Saved original XML to {original_file_path}")

            # Extract only the channel metadata and first <item> element
            xml_content_trimmed = self.extract_first_item(xml_content_original)

            if self.debug:
                trimmed_file_path = os.path.join("data", f"{self.podcast_name}_trimmed.xml")
                with open(trimmed_file_path, "w", encoding="utf-8") as f:
                    f.write(xml_content_trimmed)
                logger.debug(f"Saved trimmed XML to {trimmed_file_path}")

            # Load prompt and model from preprocess.toml
            # Assuming TomlLoader().load() returns a dict-like object or can access nested keys
            try:
                prompts_config = load_toml_file("prompts/preprocess.toml")
                system_prompt = prompts_config["podcast_extraction"]["system"]
                model_name = prompts_config["podcast_extraction"]["model"]
            except (FileNotFoundError, KeyError, TypeError) as e:
                logger.error(f"Failed to load podcast_extraction prompt/model from preprocess.toml: {e}")
                return None
            
            logger.debug(f"Sending XML content from {feed_url} to LLM (model: {model_name}) for podcast: {self.podcast_name}")
            
            # Run the synchronous api_text_completion function in a separate thread
            raw_llm_response = await asyncio.to_thread(
                api_text_completion,
                model_name,
                system_prompt,
                xml_content_trimmed # User message is the XML content
            )

            if not raw_llm_response:
                logger.error(f"LLM returned an empty response for {self.podcast_name} from {feed_url}")
                return None

            try:
                extracted_data = json.loads(raw_llm_response)
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse JSON response from LLM for {self.podcast_name}: {e}. Response: {raw_llm_response}")
                return None

            if not isinstance(extracted_data, dict):
                logger.error(f"LLM response for {self.podcast_name} is not a JSON object: {extracted_data}")
                return None

            logger.success(f"Successfully extracted podcast data using LLM for {self.podcast_name}")

            episode_id_from_llm = extracted_data.get("episode_id")
            media_url_from_llm = extracted_data.get("media_url")
            
            final_episode_id = episode_id_from_llm if episode_id_from_llm else media_url_from_llm

            return {
                "type": "podcast",
                "channel": self.podcast_name,
                "title": extracted_data.get("title", "Unknown Episode"),
                "published_at": extracted_data.get("published_at"),
                "url": media_url_from_llm,
                "episode_id": final_episode_id,
                "duration": extracted_data.get("duration"),
                "summary": extracted_data.get("summary"),
                "description": extracted_data.get("description"),
                "episode_number": extracted_data.get("episode_number")
            }

        except requests.RequestException as e:
            logger.error(f"Failed to fetch RSS feed for LLM processing for {self.podcast_name} from {feed_url}: {e}")
            return None
        except Exception as e:
            logger.error(f"An unexpected error occurred in _process_podcast_feed_llm for {self.podcast_name}: {e}", exc_info=True)
            return None

    async def check_latest_updates(self) -> None:
        """
        Check for updates in a podcast feed and cache complete episode details.
        Does not return any data; saves to cache.
        """
        cache_key = self._generate_cache_key()
        try:
            feed_url = self._get_podcast_feed_url()
            if not feed_url:
                return # Implicitly None
            result = await self._process_podcast_feed_llm(feed_url)
            if not result:
                return # Implicitly None
            self.cache.save("podcast", cache_key, result) # Cache the full episode data
            
            episode_identifier = result.get("episode_id") or result.get("url")
            if episode_identifier:
                logger.success(f"Podcast: Found and cached episode '{result.get('title')}' (ID: {episode_identifier}) for {self.podcast_name}")
            else:
                logger.warning(f"Podcast: Cached episode '{result.get('title')}' for {self.podcast_name} but no suitable episode_id or URL found for logging.")
            # No return value
        except Exception as e:
            logger.error(f"Error checking updates for podcast '{self.podcast_name}': {e}", exc_info=True)
            # No return value

    async def get_latest_update_details(self) -> Optional[Dict[str, Any]]:
        """
        Get full content and metadata for the latest podcast episode by loading from cache.
        Assumes check_latest_updates has populated the cache for this podcast channel.
        """
        cache_key = self._generate_cache_key() # Cache key for the podcast channel
        cached_episode_data = self.cache.load("podcast", cache_key)

        if cached_episode_data:
            logger.info(f"Returning cached data for podcast {self.podcast_name}, Episode Title: {cached_episode_data.get('title')}")
            return cached_episode_data
        else:
            logger.warning(f"No cached data found for podcast {self.podcast_name} (cache key: {cache_key}). Run check_latest_updates first.")
            return None

async def main():
    podcast_name = "Lenny's Podcast: Product | Growth | Career"
    # Instantiate with debug=True to test saving XML files
    connector = PodcastConnector(podcast_name, debug=True)

    logger.info(f"Demonstrating two-phase approach for podcast: {podcast_name} (Debug Mode: {connector.debug})")
    
    logger.info("=== Phase 1: Check for updates (caches data) ===")
    await connector.check_latest_updates() # Call it, no return value

    logger.info("Cache should now be populated if updates were found.")
    
    logger.info("=== Phase 2: Get content from cache/details ===")
    full_content = await connector.get_latest_update_details() # No argument
    
    if full_content:
        logger.info(f"Retrieved details: {full_content.get('title')}")
        logger.info(f"Duration: {full_content.get('duration', 'N/A')}")
        summary = full_content.get('summary') or full_content.get('description', '')
        logger.info(f"Summary/Description: {summary[:100]}...")
    else:
        logger.error(f"Error retrieving full content details for podcast: {podcast_name}. Was cache populated?")

if __name__ == "__main__":
    asyncio.run(main())
