"""
[GENERATED BY CURSOR]
Podcast Connector for fetching content from podcast channels.

This module provides functionality to:
1. Check for updates (new episodes) from podcast channels
2. Fetch detailed metadata for specific podcast episodes

The connector uses the Apple Podcasts API for searching and RSS feeds for content retrieval.
Ref: https://developer.apple.com/library/archive/documentation/AudioVideo/Conceptual/iTuneSearchAPI/index.html
"""
import requests
import xml.etree.ElementTree as ET
from utils.logging_config import logger
from utils.connector_cache import ConnectorCache
from datetime import datetime
import email.utils
from typing import Optional, Dict, Any
import asyncio
from connectors.sources.base_source import SourceConnector

class PodcastConnector(SourceConnector):
    """
    Connector for fetching content from podcast channels.
    """
    def __init__(self, podcast_name: str):
        super().__init__(source_identifier=podcast_name)
        self.podcast_name = podcast_name
        self.cache = ConnectorCache()

    def _get_podcast_feed_url(self) -> Optional[str]:
        """
        Get the RSS feed URL for a podcast by name using iTunes Search API.
        """
        base_url = "https://itunes.apple.com/search"
        
        # Check if the podcast name contains Chinese characters
        # If so, use the CN country code, otherwise use US
        country = "CN" if any('\u4e00' <= char <= '\u9fff' for char in self.podcast_name) else "US"
        logger.debug(f"Using country={country} for podcast: {self.podcast_name}")
        
        params = {
            "term": self.podcast_name, 
            "entity": "podcast", 
            "limit": 1,
            "country": country,
        }

        try:
            response = requests.get(base_url, params=params)
            response.raise_for_status() # Raise an exception for HTTP errors
            results = response.json().get('results')
            return results[0]['feedUrl']
        except requests.RequestException as e:
            logger.error(f"Error fetching podcast feed URL for {self.podcast_name}: {e}")
            return None
        except (KeyError, IndexError) as e:
            logger.error(f"Error parsing iTunes API response for {self.podcast_name}: {e}")
            return None 

    def _process_podcast_feed(self, feed_url: str) -> Optional[Dict[str, Any]]:
        """
        Process a podcast RSS feed and extract the latest episode details.
        """
        try:
            response = requests.get(feed_url)
            response.raise_for_status()
            root = ET.fromstring(response.content)
            channel = root.find('channel')
            if channel is None:
                logger.warning(f"No channel found in RSS feed: {feed_url}")
                return None
            latest_item = channel.find('item')
            if latest_item is None:
                logger.warning(f"No episodes found in podcast feed: {self.podcast_name}")
                return None

            data = {}
            # Try to find specific iTunes tags first
            for child in latest_item:
                tag_name = child.tag.split('}')[-1] if '}' in child.tag else child.tag
                if tag_name == 'title':
                    data['title'] = child.text
                elif tag_name == 'pubDate':
                    data['published_at_raw'] = child.text
                elif tag_name == 'duration' and 'itunes' in child.tag.lower():
                    data['duration'] = child.text
                elif tag_name == 'summary' and 'itunes' in child.tag.lower():
                    data['summary'] = child.text
                elif tag_name == 'description': # generic description
                    data['description_generic'] = child.text
                elif tag_name == 'episode' and 'itunes' in child.tag.lower() and 'episodetype' not in child.tag.lower():
                    data['episode_number'] = child.text
                elif tag_name == 'guid':
                    data['episode_id'] = child.text
                elif child.tag.endswith('link') and not data.get('url'): #Prefer specific link from enclosure if available
                     data['url'] = child.text

            # Enclosure for media URL
            enclosure_tag = latest_item.find('enclosure')
            if enclosure_tag is not None and 'url' in enclosure_tag.attrib:
                data['url'] = enclosure_tag.attrib['url'] 
            elif latest_item.find('link') is not None and not data.get('url') : # Fallback to item link if no enclosure
                data['url'] = latest_item.find('link').text

            # Fallbacks and formatting
            if 'summary' not in data and 'description_generic' in data:
                data['summary'] = data['description_generic']
            
            published_at_iso = datetime.now().date().isoformat()
            if data.get('published_at_raw'):
                try:
                    dt = email.utils.parsedate_to_datetime(data['published_at_raw'])
                    published_at_iso = dt.date().isoformat()
                except Exception as e:
                    logger.warning(f"Could not parse pubDate '{data['published_at_raw']}' for {self.podcast_name}: {e}")
            
            return {
                "type": "podcast",
                "channel": self.podcast_name,
                "title": data.get('title', "Unknown Episode"),
                "published_at": published_at_iso,
                "url": data.get('url'),
                "episode_id": data.get('episode_id', data.get('url')), 
                "duration": data.get('duration'),
                "summary": data.get('summary'),
                "description": data.get('description_generic', data.get('summary')),
                "episode_number": data.get('episode_number')
            }
        except requests.RequestException as e:
            logger.error(f"Failed to fetch or process RSS feed for {self.podcast_name} from {feed_url}: {e}")
            return None
        except ET.ParseError as e:
            logger.error(f"Error parsing XML for {self.podcast_name} from {feed_url}: {e}")
            return None

    async def check_latest_updates(self) -> None:
        """
        Check for updates in a podcast feed and cache complete episode details.
        Does not return any data; saves to cache.
        """
        cache_key = self._generate_cache_key()
        try:
            feed_url = self._get_podcast_feed_url()
            if not feed_url:
                return # Implicitly None
            result = self._process_podcast_feed(feed_url)
            if not result:
                return # Implicitly None
            self.cache.save("podcast", cache_key, result) # Cache the full episode data
            
            episode_identifier = result.get("episode_id") or result.get("url")
            if episode_identifier:
                logger.success(f"Podcast: Found and cached episode '{result.get('title')}' (ID: {episode_identifier}) for {self.podcast_name}")
            else:
                logger.warning(f"Podcast: Cached episode '{result.get('title')}' for {self.podcast_name} but no suitable episode_id or URL found for logging.")
            # No return value
        except Exception as e:
            logger.error(f"Error checking updates for podcast '{self.podcast_name}': {e}", exc_info=True)
            # No return value

    async def get_latest_update_details(self) -> Optional[Dict[str, Any]]:
        """
        Get full content and metadata for the latest podcast episode by loading from cache.
        Assumes check_latest_updates has populated the cache for this podcast channel.
        """
        cache_key = self._generate_cache_key() # Cache key for the podcast channel
        cached_episode_data = self.cache.load("podcast", cache_key)

        if cached_episode_data:
            logger.info(f"Returning cached data for podcast {self.podcast_name}, Episode Title: {cached_episode_data.get('title')}")
            return cached_episode_data
        else:
            logger.warning(f"No cached data found for podcast {self.podcast_name} (cache key: {cache_key}). Run check_latest_updates first.")
            return None

async def main():
    podcast_name = "一席"
    connector = PodcastConnector(podcast_name)

    logger.info(f"Demonstrating two-phase approach for podcast: {podcast_name}")
    
    logger.info("=== Phase 1: Check for updates (caches data) ===")
    await connector.check_latest_updates() # Call it, no return value

    logger.info("Cache should now be populated if updates were found.")
    
    logger.info("=== Phase 2: Get content from cache/details ===")
    full_content = await connector.get_latest_update_details() # No argument
    
    if full_content:
        logger.info(f"Retrieved details: {full_content.get('title')}")
        logger.info(f"Duration: {full_content.get('duration', 'N/A')}")
        summary = full_content.get('summary') or full_content.get('description', '')
        logger.info(f"Summary/Description: {summary[:100]}...")
    else:
        logger.error(f"Error retrieving full content details for podcast: {podcast_name}. Was cache populated?")

if __name__ == "__main__":
    asyncio.run(main())
