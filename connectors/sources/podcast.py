"""
[GENERATED BY CURSOR]
Podcast Connector for fetching content from podcast channels.

This module provides functionality to:
1. Check for updates (new episodes) from podcast channels
2. Fetch detailed metadata for specific podcast episodes

The connector uses the Apple Podcasts API for searching and RSS feeds for content retrieval.
Ref: https://developer.apple.com/library/archive/documentation/AudioVideo/Conceptual/iTuneSearchAPI/index.html
"""
import requests
import xml.etree.ElementTree as ET
from utils.logging_config import logger
from utils.connector_cache import ConnectorCache
from datetime import datetime
import email.utils

def generate_cache_key(podcast_name):
    """
    Generate a standardized cache key for podcast data.
    
    Args:
        podcast_name (str): The name of the podcast
        
    Returns:
        str: A standardized cache key
    """
    # Convert podcast name to lowercase and replace spaces with underscores
    normalized_name = podcast_name.lower().replace(' ', '_')
    
    # Return the normalized name as the cache key
    # Note: The date will be added by ConnectorCache
    return normalized_name

def get_podcast_feed_url(podcast_name):
    """
    Get the RSS feed URL for a podcast by name using iTunes Search API.
    
    Args:
        podcast_name (str): Name of the podcast channel
        
    Returns:
        str: RSS feed URL or None if not found
    """
    # Format the URL for iTunes Search API
    base_url = "https://itunes.apple.com/search"

    # Check the podcast name and decide the country
    # Now support: Chinese (CN) and English (US)
    if any('\u4e00' <= char <= '\u9fff' for char in podcast_name):
        country = "CN"
        logger.debug(f"Detected Chinese podcast: {podcast_name}, using country=CN")
    else:
        country = "US"
        logger.debug(f"Using default country=US for podcast: {podcast_name}")
    
    # Add country parameter to the API request
    params = {
        "term": podcast_name,
        "entity": "podcast",
        "limit": 1,
        "country": country
    }
    
    # Get the podcast information
    response = requests.get(base_url, params=params)
    if response.status_code != 200 or not response.json().get('results'):
        logger.warning(f"No podcast found for name: {podcast_name}")
        return None
        
    # Get the RSS feed URL
    feed_url = response.json()['results'][0]['feedUrl']
    return feed_url

def process_podcast_feed(feed_url, podcast_name):
    """
    Process a podcast RSS feed and extract the latest episode details.
    
    Args:
        feed_url (str): URL of the podcast RSS feed
        podcast_name (str): Name of the podcast channel
        
    Returns:
        dict: Metadata for the latest podcast episode or None if not found
    """
    # Get the RSS feed content
    response = requests.get(feed_url)
    if response.status_code != 200:
        logger.error(f"Failed to fetch RSS feed for podcast: {podcast_name}")
        return None
    
    # Parse the XML feed
    root = ET.fromstring(response.content)
    channel = root.find('channel')
    latest_item = channel.find('item')
    
    if latest_item is None:
        logger.warning(f"No episodes found in podcast feed: {podcast_name}")
        return None
    
    # Initialize variables
    title = published_at = duration = summary = link = description = episode = None
    
    # Extract link from enclosure tag if present
    enclosure_tag = latest_item.find('enclosure')
    if enclosure_tag is not None and 'url' in enclosure_tag.attrib:
        link = enclosure_tag.attrib['url']
    
    # Parse other details with full metadata extraction
    for child in latest_item:
        tag_name = child.tag.split('}')[-1]  # Handle potential namespaces
        
        if tag_name == 'title':
            title = child.text
        elif tag_name == 'pubDate':
            published_at = child.text
        elif tag_name == 'duration' and 'itunes' in child.tag:
            duration = child.text
        elif tag_name == 'summary' and 'itunes' in child.tag:
            summary = child.text
        elif tag_name == 'description':
            description = child.text
        elif tag_name == 'episode' and 'itunes' in child.tag and 'episodeType' not in child.tag:
            episode = child.text
        elif tag_name == 'guid':
            episode_id = child.text
        elif tag_name == 'duration' and duration is None:
            duration = child.text
        elif tag_name == 'summary' and summary is None:
            summary = child.text
    
    # Use description as summary if summary is still None
    if summary is None:
        summary = description
    
    # Format the published date
    if published_at:
        try:
            # Parse the RFC 822 date format
            dt = email.utils.parsedate_to_datetime(published_at)
            # Set only the date part
            published_at = dt.date().isoformat()
        except (ValueError, TypeError) as e:
            logger.error(f"Could not parse published_at date for podcast '{podcast_name}': {e}")
            published_at = datetime.now().date().isoformat()
    else:
        published_at = datetime.now().date().isoformat()
    
    # Create comprehensive result with all available metadata
    result = {
        "type": "podcast",
        "channel": podcast_name,
        "title": title or "Unknown Episode",
        "published_at": published_at,
        "url": link,
        "episode_id": episode_id if 'episode_id' in locals() else link,  # Use guid or url as identifier
        "duration": duration,
        "summary": summary,
        "description": description,
        "episode": episode
    }
    
    return result

def check_latest_updates(podcast_name):
    """
    Check for updates in a podcast feed and cache complete episode details.
    This function orchestrates the podcast data retrieval process.
    
    Args:
        podcast_name (str): Name of the podcast channel
        
    Returns:
        dict: Metadata for the latest podcast episode or None if not found
    """
    # Initialize cache
    cache = ConnectorCache()
    cache_key = generate_cache_key(podcast_name)
    
    try:
        # Step 1: Get the podcast feed URL
        feed_url = get_podcast_feed_url(podcast_name)
        if not feed_url:
            return None
            
        # Step 2: Process the feed and get episode details
        result = process_podcast_feed(feed_url, podcast_name)
        if not result:
            return None
        
        # Step 3: Cache the complete result for later
        cache.save("podcast", cache_key, result)
        
        return result
        
    except Exception as e:
        logger.error(f"Error checking updates for podcast '{podcast_name}': {e}")
        return None

def get_latest_update_details(podcast_name):
    """
    Get full content and metadata for the latest podcast episode from cache.
    Does not implement fetching logic - only retrieves from cache.
    
    Args:
        podcast_name: Podcast name for cache lookup
    
    Returns:
        dict: Complete metadata for the episode
        
    Raises:
        ValueError: If episode not found in cache
    """
    if not podcast_name:
        raise ValueError("Podcast name is required to get episode content from cache")
    
    # Try to get from cache
    cache = ConnectorCache()
    cache_key = generate_cache_key(podcast_name)
    cached_data = cache.load("podcast", cache_key)
    
    # Check if we found cached data
    if cached_data:
        logger.info(f"Using cached data for podcast episode: {cached_data.get('title')}")
        return cached_data
    
    # If we reach here, the episode was not in cache
    error_msg = f"No episode found in cache for podcast {podcast_name}"
    logger.error(error_msg)
    raise ValueError(error_msg)

def main():
    """Example demonstrating the two-phase podcast content retrieval approach."""
    podcast_name = "一席"
    
    # PHASE 1: Check for updates
    print("\n=== Phase 1: Check for updates ===")
    print("In this phase, we check for new episodes and cache complete data.")
    latest = check_latest_updates(podcast_name)
    
    if not latest:
        print("No episodes found")
        return
        
    print(f"Found latest episode: {latest['title']}")
    print(f"Published: {latest['published_at']}")
    print(f"URL: {latest['url']}")
    
    # PHASE 2: Get full content from cache
    print("\n=== Phase 2: Get content from cache ===")
    print("In this phase, we retrieve the full content from cache without re-fetching.")
    try:
        full_content = get_latest_update_details(podcast_name)
        print(f"Retrieved from cache: {full_content['title']}")
        print(f"Duration: {full_content.get('duration', 'N/A')}")
        if full_content.get('description'):
            print(f"Description: {full_content['description'][:100]}...")
        if full_content.get('summary'):
            print(f"Summary: {full_content['summary'][:100]}...")
    except ValueError as e:
        print(f"Error retrieving from cache: {e}")


if __name__ == "__main__":
    main()
