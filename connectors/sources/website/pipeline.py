"""
[GENERATED BY CURSOR]
Workflow orchestrator for website content processing.
Combines gateway url discovery with content extraction in a complete pipeline.

This module implements a two-phase content retrieval approach:
Phase 1: Check for updates by scraping the gateway page and finding the latest content URL (check_latest_updates)
Phase 2: Process and retrieve the full content details when needed (get_latest_update_details)
"""

import asyncio
from utils.logging_config import logger
from connectors.sources.website import gateway
from connectors.sources.website import content
from typing import Dict, Any, Optional
from utils.connector_cache import ConnectorCache
import tomllib
from functools import lru_cache
from utils.url_utils import extract_base_url
from connectors.sources.base_source import SourceConnector

@lru_cache(maxsize=None)
def _load_website_configs() -> Dict[str, Any]:
    """Loads website configurations from config/website.toml.
    The result is cached, so the file is read and parsed only once.
    Raises FileNotFoundError or tomllib.TOMLDecodeError if issues occur on first load.
    """
    try:
        with open("config/website.toml", "rb") as f:
            loaded_configs = tomllib.load(f)
        logger.info(f"Successfully loaded website configurations from config/website.toml for {len(loaded_configs)} base URLs (cached).")
        return loaded_configs
    except FileNotFoundError:
        logger.error("Configuration file config/website.toml not found. This error will be cached.")
        raise
    except tomllib.TOMLDecodeError as e:
        logger.error(f"Error decoding config/website.toml: {e}. This error will be cached.")
        raise

class WebsiteConnector(SourceConnector):
    """
    Connector for fetching and processing content from websites.
    Orchestrates a two-phase approach: checking for latest updates (Phase 1)
    and retrieving full content details (Phase 2).
    """
    def __init__(self, channel: str, source_url: str):
        super().__init__(source_identifier=channel) # Call base class __init__
        self.channel = channel # Kept for existing logic
        self.source_url = source_url
        self.cache = ConnectorCache()
        self.website_config: Optional[Dict[str, Any]] = self._prepare_website_processing_config()
        if not self.website_config:
            # Error logged in _prepare_website_processing_config
            raise ValueError(f"Failed to prepare website config for channel {channel} and source {source_url}")

    def _prepare_website_processing_config(self) -> Optional[Dict[str, Any]]:
        """Prepares and validates necessary configurations for website processing."""
        try:
            all_website_configs = _load_website_configs()
            if not all_website_configs:
                logger.error("Failed to load website configurations or config is empty.")
                return None
        except (FileNotFoundError, tomllib.TOMLDecodeError) as e:
            logger.error(f"Critical error loading config/website.toml for channel {self.channel}: {e}")
            return None

        base_url = extract_base_url(self.source_url)
        if not base_url or base_url not in all_website_configs:
            logger.error(f"No configuration found for base_url '{base_url}' (derived from {self.source_url}) for channel '{self.channel}'. Ensure config/website.toml has an entry for this base URL.")
            return None
            
        site_config = all_website_configs[base_url].copy()
        logger.info(f"Loaded scraper parameters for base_url {base_url} (channel {self.channel}): {site_config}")

        required_keys = ["gateway_scraper", "content_scraper", "gateway_parser", "content_parser"]
        missing_keys = [key for key in required_keys if key not in site_config]

        if missing_keys:
            error_msg = f"Configuration for base_url '{base_url}' in config/website.toml is missing keys: {missing_keys}. (Channel: '{self.channel}', Source: {self.source_url})"
            logger.error(error_msg)
            return None
        return site_config

    async def check_latest_updates(self) -> None:
        """
        Asynchronously checks a website source for its latest content URL and caches basic findings (Phase 1).
        Does not return any data; saves to cache.
        """
        if not self.website_config:
            logger.error(f"Website config not available for channel {self.channel}. Cannot check updates.")
            return # Implicitly None

        cache_key = self._generate_cache_key()
        logger.info(f"Checking for updates from website: {self.source_url} for channel '{self.channel}'")

        try:
            latest_article_info = await asyncio.to_thread(
                gateway.find_latest_release,
                url=self.source_url,
                scraper_type=self.website_config["gateway_scraper"],
                gateway_parser_name=self.website_config.get("gateway_parser")
            )

            if not latest_article_info or not latest_article_info.get('url'):
                logger.warning(f"gateway.find_latest_release returned no information or no URL for {self.source_url}. Channel: '{self.channel}'.")
                return # Implicitly None

            phase1_result = {
                "channel": self.channel,
                "type": "website",
                "published_at": latest_article_info.get('published_at'),
                "url": latest_article_info.get('url'),
                "source_url": self.source_url,
            }
            
            self.cache.save("website", cache_key, phase1_result) # Cache the phase 1 result
            content_url_cached = phase1_result.get('url')
            logger.success(f"Website: Found and cached latest content info for {self.channel}: URL {content_url_cached}")
            # No return value
        except Exception as e:
            logger.error(f"Error during check_latest_updates for {self.channel} ({self.source_url}): {e}", exc_info=True)
            # No return value

    async def get_latest_update_details(self) -> Optional[Dict[str, Any]]:
        """
        Asynchronously retrieves and processes the full details of the latest content update for the channel.
        It relies on check_latest_updates having cached the Phase 1 data (including the content URL).
        """
        if not self.website_config:
            logger.error(f"Website config not available for channel {self.channel}. Cannot get update details.")
            return None
        
        cache_key_for_channel = self._generate_cache_key()
        cached_phase1_data = self.cache.load("website", cache_key_for_channel)

        if not cached_phase1_data or not cached_phase1_data.get('url'):
            logger.warning(f"No valid Phase 1 cached data (or missing URL) found for channel {self.channel} (cache key {cache_key_for_channel}). Run check_latest_updates first.")
            return None

        content_url_str = cached_phase1_data.get('url')
        published_at_from_cache = cached_phase1_data.get('published_at')
        source_url_from_cache = cached_phase1_data.get('source_url', self.source_url) # Fallback

        logger.info(f"Getting latest update details for channel '{self.channel}' using cached content URL: {content_url_str}")

        try:
            content_result = await asyncio.to_thread(
                content.scrape_and_process_content,
                url=content_url_str,
                scraper_type=self.website_config["content_scraper"],
                content_parser=self.website_config["content_parser"]
            )
            
            if not content_result:
                logger.error(f"Failed to process latest content for {self.channel} from URL: {content_url_str}")
                return None
            
            result = {
                "title": content_result.get('title'),
                "channel": self.channel,
                "type": "website",
                "published_at": published_at_from_cache,
                "url": content_url_str,
                "duration": content_result.get('read_time'),
                "summary": content_result.get('summary'),
                "full_text_markdown": content_result.get('markdown_content'),
                "source_url": source_url_from_cache
            }
            
            logger.success(f"Successfully retrieved content details for {self.channel} from URL: {content_url_str}")
            return result
            
        except Exception as e:
            logger.error(f"Error retrieving content details for {self.channel} ({content_url_str}): {e}", exc_info=True)
            return None

async def main():
    """Demonstrate the two-phase website content retrieval approach."""
    example_subscription = {
        "channel": "远川科技评论 (36氪)",
        "source_url": "https://36kr.com/user/5060931",
    }
    
    channel = example_subscription["channel"]
    source_url = example_subscription["source_url"]

    logger.info(f"Demonstrating two-phase approach for website channel: {channel} ({source_url})")

    try:
        connector = WebsiteConnector(channel=channel, source_url=source_url)
    except ValueError as e:
        logger.error(f"Error initializing connector: {e}")
        return
    except (FileNotFoundError, tomllib.TOMLDecodeError) as e:
        logger.error(f"Critical error loading website configurations: {e}")
        return

    logger.info("\n=== Phase 1: Check for updates (caches data) ===")
    await connector.check_latest_updates() # Call it, no return value
    
    logger.info("Cache should now be populated if updates were found.")

    logger.info("\n=== Phase 2: Get content details from cache ===")
    full_content_details = await connector.get_latest_update_details() # No argument
    
    if full_content_details:
        logger.info(f"Retrieved full content for {channel}:")
        logger.info(f"  Title: {full_content_details.get('title', 'N/A')}")
        logger.info(f"  Published: {full_content_details.get('published_at', 'N/A')}")
        logger.info(f"  URL: {full_content_details.get('url')}")
        logger.info(f"  Est. Read Time: {full_content_details.get('duration', 'N/A')}")
        summary = full_content_details.get('summary', '')
        logger.info(f"  Summary: {summary[:200] + '...' if summary and len(summary) > 200 else summary}")
    else:
        logger.error(f"Error retrieving full content details for {channel}. Was cache populated?")

if __name__ == "__main__":
    asyncio.run(main()) 