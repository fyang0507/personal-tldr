"""
[GENERATED BY CURSOR]
Basic Website Scraper

This module provides a simple website scraping utility using the `requests` library.
It is designed for fetching HTML content from websites that do not employ heavy
JavaScript rendering or sophisticated anti-bot mechanisms.
"""

import requests
from pathlib import Path
from utils.logging_config import logger
from typing import Optional, Tuple
from .base import BaseScraper


class BasicScraper(BaseScraper):
    """
    A basic scraper that uses the `requests` library to fetch HTML content.
    """

    def scrape(self, url: str) -> Optional[str]:
        """
        Fetches a webpage using the requests library.
        
        Args:
            url: URL to scrape
            
        Returns:
            tuple: (html_content, status_code) or (None, None) if an error occurs
        """
        try:
            logger.info(f"Using Basic Scraper to fetch webpage: {url}")
            
            # Set headers to mimic a browser
            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.9",
            }
            
            # Fetch the webpage
            response = requests.get(url, headers=headers, timeout=30)
            
            # Check if the request was successful
            if response.status_code >= 400:
                logger.error(f"Got HTTP error {response.status_code} for {url}")
                return None
            
            logger.info(f"Successfully fetched {url} with status code: {response.status_code}")
            return response.text
            
        except requests.exceptions.RequestException as e:
            logger.error(f"Error fetching {url}: {e}")
            return None


def main():
    """Main function with placeholder values."""
    # Target URL from the user
    target_url = "https://36kr.com/user/5294208"
    output_dir = "data/36kr"
    
    # Ensure output directory exists
    Path(output_dir).mkdir(parents=True, exist_ok=True)
        
    html_path = Path(output_dir) / f"demo_basic_scraper.html"
    
    # Use the class-based scraper
    scraper = BasicScraper()
    html_content = scraper.scrape(url=target_url)
    
    if html_content:
        # Save HTML
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        logger.success(f"Saved HTML content to {html_path}")
    else:
        logger.error("Failed to scrape the website.")


if __name__ == "__main__":
    main() 