"""
[GENERATED BY CURSOR]
A module to fetch the most recent videos from a Bilibili user.

This module provides functionality to:
1. Check for updates from Bilibili users without downloading full content
2. Fetch detailed metadata for specific Bilibili videos

The connector uses the Bilibili API to retrieve video data from a specified user
by their mid (member ID). Videos are sorted by publication date (newest first).

This implementation uses curl to fetch the data. Standard implementation to fetch the data is likely to encounter rate limit, risk of being blocked, etc.

Ref: https://github.com/SocialSisterYi/bilibili-API-collect/blob/master/docs/video/collection.md#%E6%A0%B9%E6%8D%AE%E5%85%B3%E9%94%AE%E8%AF%8D%E6%9F%A5%E6%89%BE%E8%A7%86%E9%A2%91
Note: There is no description for the video using this API. To get the description, another API is needed. See: https://github.com/SocialSisterYi/bilibili-API-collect/blob/master/docs/video/info.md#%E8%A7%86%E9%A2%91%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF
"""

from datetime import datetime
import json
import subprocess
from utils.logging_config import logger
from typing import Dict, Any, Optional
from utils.connector_cache import ConnectorCache
import asyncio
from connectors.sources.base_source import SourceConnector


class BilibiliConnector(SourceConnector):
    """
    Connector for fetching the most recent videos from a Bilibili user.
    """
    USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Safari/605.1.15"

    def __init__(self, uid: int, channel: str):
        super().__init__(source_identifier=channel)
        self.uid = uid
        self.channel = channel
        self.cache = ConnectorCache()

    async def check_latest_updates(self) -> None:
        """
        Check for updates from a Bilibili user and cache the latest video metadata.
        Does not return any data; saves to cache.
        """
        cache_key = self._generate_cache_key()
        
        try:
            response = self._get_user_videos(self.uid, page_size=1)
            
            if response.get("code") != 0:
                logger.error(f"Error fetching videos for Bilibili user '{self.channel}': {response.get('message', 'Unknown error')}")
                return
                
            if "data" not in response or "archives" not in response["data"]:
                logger.error(f"Unexpected response structure for Bilibili user '{self.channel}'")
                return
                
            videos = response["data"]["archives"]
            if not videos:
                logger.warning(f"No videos found for Bilibili user '{self.channel}'")
                return
            
            latest_video = videos[0]
            result = self._format_video_data(latest_video)
            if not result:
                logger.error(f"Error formatting video data for Bilibili user '{self.channel}'")
                return
            
            result["type"] = "bilibili"
            result["channel"] = self.channel
            
            self.cache.save("bilibili", cache_key, result)
            logger.success(f"Bilibili: Found and cached video '{result.get('title')}' (BVID: {result.get('bvid')}) for channel {self.channel}")
            
        except Exception as e:
            logger.error(f"Error checking updates for Bilibili user '{self.channel}': {e}", exc_info=True)

    async def get_latest_update_details(self) -> Optional[Dict[str, Any]]:
        """
        Get full content and metadata for the latest Bilibili video by loading from cache.
        Assumes check_latest_updates has populated the cache for this channel.
        """
        cache_key = self._generate_cache_key()
        cached_channel_data = self.cache.load("bilibili", cache_key)

        if cached_channel_data:
             logger.info(f"Returning cached data for Bilibili channel {self.channel}, BVID: {cached_channel_data.get('bvid')}")
             return cached_channel_data
        else:
            logger.warning(f"No cached data found for Bilibili channel {self.channel} (cache key: {cache_key}). Run check_latest_updates first.")
            return None

    def _get_user_videos(self, mid: int, page_size: int = 5, page_num: int = 1) -> Dict[str, Any]:
        """
        Fetch videos for a specific Bilibili user using curl.
        """
        url = "https://api.bilibili.com/x/series/recArchivesByKeywords"
        curl_cmd = [
            "curl", "-s", "-G", url,
            "--data-urlencode", f"mid={mid}",
            "--data-urlencode", "keywords=",
            "--data-urlencode", f"ps={page_size}",
            "--data-urlencode", f"pn={page_num}",
            "--data-urlencode", "orderby=pubdate"
        ]
        logger.info(f"Executing curl command: {' '.join(curl_cmd)}")
        try:
            result = subprocess.run(curl_cmd, capture_output=True, text=True, check=True)
            if result.stderr:
                logger.warning(f"Curl stderr: {result.stderr}")
            response = json.loads(result.stdout)
            logger.info(f"Response code: {response.get('code')}")
            logger.info(f"Response message: {response.get('message')}")
            return response
        except subprocess.CalledProcessError as e:
            logger.error(f"Curl command failed with exit code {e.returncode}")
            logger.error(f"Stderr: {e.stderr}")
            return {"code": -1, "message": f"Curl command failed: {e}", "data": {"archives": [], "page": {"total": 0}}}
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON response: {e}")
            logger.error(f"Response content: {result.stdout[:500]}...")
            return {"code": -1, "message": f"JSON parsing failed: {e}", "data": {"archives": [], "page": {"total": 0}}}

    def _format_video_data(self, video: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Extract and format relevant video information.
        """
        try:
            logger.info(f"Formatting video data: {video}")
            return {
                "aid": video.get("aid"),
                "bvid": video.get("bvid"),
                "title": video.get("title"),
                "published_at": datetime.fromtimestamp(video.get("pubdate", 0)).date().isoformat() if video.get("pubdate") else None,
                "duration": video.get("duration"),
                "stats": {
                    "view_count": video.get("stat", {}).get("view", 0),
                },
                "thumbnail": video.get("pic"),
                "url": f"https://www.bilibili.com/video/{video.get('bvid')}"
            }
        except Exception as e:
            logger.error(f"Error formatting video data: {e}")
            logger.error(f"Video data: {json.dumps(video, indent=2, ensure_ascii=False)[:500]}...")
            return None

async def main():
    """Example demonstrating the two-phase Bilibili content retrieval approach."""
    uid = 946974
    channel = "影视飓风"
    
    connector = BilibiliConnector(uid, channel)

    logger.info(f"Demonstrating two-phase approach for Bilibili channel: {channel}")
    
    logger.info("=== Phase 1: Check for updates (caches data) ===")
    await connector.check_latest_updates()
    
    logger.info("Cache should now be populated if updates were found.")

    logger.info("=== Phase 2: Get content from cache/details ===")
    full_content = await connector.get_latest_update_details()
    
    if full_content:
        logger.info(f"Retrieved details: {full_content.get('title')}")
        logger.info(f"Duration: {full_content.get('duration', 'N/A')}s")
        logger.info(f"URL: {full_content.get('url')}")
        logger.info(f"Views: {full_content.get('stats', {}).get('view_count', 'N/A')}")
    else:
        logger.error(f"Error retrieving full content details for Bilibili channel: {channel}. Was cache populated?")


if __name__ == "__main__":
    asyncio.run(main()) 