"""
[GENERATED BY CURSOR]
Script to scrape websites using the requests library.
Provides basic scraping functionality for sites without anti-bot protection.
"""

import requests
from pathlib import Path
from utils.logging_config import logger
from markdownify import markdownify
from readability import Document
from urllib.parse import urlparse


def fetch_webpage(url: str):
    """
    Fetches a webpage using the requests library.
    
    Args:
        url: URL to scrape
        
    Returns:
        tuple: (html_content, status_code) or (None, None) if an error occurs
    """
    try:
        logger.info(f"Fetching webpage: {url}")
        
        # Set headers to mimic a browser
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
        }
        
        # Fetch the webpage
        response = requests.get(url, headers=headers, timeout=30)
        
        # Check if the request was successful
        if response.status_code >= 400:
            logger.error(f"Got HTTP error {response.status_code} for {url}")
            return None, response.status_code
        
        logger.info(f"Successfully fetched {url} with status code: {response.status_code}")
        return response.text, response.status_code
        
    except requests.exceptions.RequestException as e:
        logger.error(f"Error fetching {url}: {e}")
        return None, None


def parse_html_to_markdown(html_content):
    """
    Parses HTML content to Markdown using readability and markdownify.
    
    Args:
        html_content: The HTML content to parse
        
    Returns:
        tuple: (markdown_content, title) or (None, None) if an error occurs
    """
    try:
        # Process content with readability
        doc = Document(html_content)
        title = doc.title()
        main_content = doc.summary()
        
        # Convert to markdown
        markdown_content = markdownify(main_content)
        
        return markdown_content, title
    except Exception as e:
        logger.error(f"Error parsing HTML to Markdown: {e}")
        return None, None


def scrape_website_to_markdown(url: str):
    """
    Scrapes a website using requests and converts content to Markdown.
    
    Args:
        url: URL to scrape
        
    Returns:
        tuple: (html_content, markdown_content, title) or (None, None, None) if an error occurs
    """
    try:
        # Step 1: Fetch HTML content
        html_content, status_code = fetch_webpage(url)
        
        if not html_content:
            return None, None, None
            
        # Step 2: Parse HTML to Markdown
        markdown_content, title = parse_html_to_markdown(html_content)
        
        if not markdown_content:
            return html_content, None, None
        
        return html_content, markdown_content, title
            
    except Exception as e:
        logger.error(f"Error scraping {url}: {e}")
        return None, None, None


def main():
    """Main function with placeholder values."""
    # Target URL from the user
    target_url = "https://36kr.com/user/5294208"
    output_dir = "data/36kr/basic"
    
    # Ensure output directory exists
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # Generate file paths
    domain = urlparse(target_url).netloc
    path = urlparse(target_url).path.replace('/', '_')
    if path.startswith('_'):
        path = path[1:]
    if path.endswith('_'):
        path = path[:-1]
        
    # Limit filename length
    if len(path) > 50:
        path = path[:50]
        
    output_filepath = Path(output_dir) / f"{domain}_{path}.md"
    html_path = str(output_filepath).replace('.md', '.html')
    
    # Scrape website
    html_content, markdown_content, title = scrape_website_to_markdown(
        url=target_url
    )
    
    if markdown_content:
        # Save HTML
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        logger.success(f"Saved HTML content to {html_path}")
        
        # Save Markdown
        with open(output_filepath, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        logger.success(f"Saved Markdown content to {output_filepath}")
        
        logger.success(f"Successfully scraped article: {title}")
        logger.info(f"Content saved to {output_dir}")
    else:
        logger.error("Failed to scrape the website.")


if __name__ == "__main__":
    main() 