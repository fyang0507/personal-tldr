"""
[GENERATED BY CURSOR]
Common utilities for website scrapers.
Contains shared functionality used by both basic and playwright scrapers.
"""

import json
from utils.logging_config import logger
from bs4 import BeautifulSoup
from pathlib import Path


def extract_catalogue(html_content: str, base_url: str) -> str:
    """
    Parses HTML content from a catalog-like page to extract article details.
    Designed to work with structures similar to the 36kr author page.

    Args:
        html_content: The HTML content of the page.
        base_url: The base URL to prepend to relative article URLs (e.g., "https://36kr.com").

    Returns:
        list: A list of dictionaries, where each dictionary contains:
              'title' (str): The title of the article.
              'url' (str): The absolute URL of the article.
              'published_date' (str): The publication date string.
              Returns an empty list if an error occurs or no articles are found.
    """
    articles = []
    if not html_content:
        logger.warning("Received empty HTML content.")
        return articles

    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Selector based on the provided demo_gateway.html structure (e.g., 36kr)
        # It targets items within a flow list.
        article_items = soup.select('div.flow-item div.kr-flow-article-item')

        if not article_items:
            # Check if the body exists to avoid logging on completely empty/invalid HTML
            if soup.body:
                logger.info("No article items found using selector 'div.flow-item div.kr-flow-article-item'.")
            else:
                logger.warning("HTML content seems empty or invalid, no body tag found.")
            return articles # Early exit if no items found

        for item_idx, item in enumerate(article_items):
            title_tag = item.select_one('a.article-item-title')
            # Date is typically in a span with class 'kr-flow-bar-time' inside a 'div.kr-flow-bar'
            date_tag = item.select_one('div.kr-flow-bar span.kr-flow-bar-time')
            
            title = None
            url = None
            published_date = None

            if title_tag:
                title = title_tag.get_text(strip=True)
                raw_url = title_tag.get('href')
                if raw_url:
                    if raw_url.startswith(('http://', 'https://')):
                        url = raw_url
                    else:
                        # Properly join base_url and relative path
                        url = f"{base_url.rstrip('/')}/{raw_url.lstrip('/')}"
            
            if date_tag:
                # Extracts the last piece of stripped text, assuming it's the date
                # E.g., <span ...><i ...></i>2025-04-25</span> results in "2025-04-25"
                date_contents = list(date_tag.stripped_strings)
                if date_contents:
                    published_date = date_contents[-1]
                else:
                    # Fallback if no stripped_strings (e.g. no nested tags, just text)
                    published_date_direct_text = date_tag.get_text(strip=True)
                    if published_date_direct_text:
                        published_date = published_date_direct_text
                    else:
                        logger.debug(f"Article item {item_idx}: Date tag found but no text content could be extracted: {str(date_tag)[:100]}")

            if title and url and published_date:
                articles.append({
                    'title': title,
                    'url': url,
                    'published_date': published_date
                })
            else:
                # Log if an item that seemed like an article couldn't be fully parsed.
                # This is useful for debugging selectors or unexpected HTML variations.
                if title_tag or date_tag: # Log only if we found some indication of an article
                    missing_parts = []
                    if not title: missing_parts.append("title")
                    if not url: missing_parts.append("url") # URL depends on title_tag and its href
                    if not published_date: missing_parts.append("date")
                    
                    logger.warning(
                        f"Article item {item_idx}: Partial data. Missing: {', '.join(missing_parts) or 'unknown'}. "
                        f"Title found: {bool(title)}, URL found: {bool(url)}, Date found: {bool(published_date)}. "
                        f"Item snippet: {str(item)[:200]}"
                    )

        if not articles and article_items: # Found HTML elements but failed to parse any into structured data
             logger.warning("HTML elements matched article selectors, but no complete article entries were successfully parsed.")
        
    except Exception as e:
        logger.error(f"Error during HTML parsing for articles: {e}", exc_info=True)
        # Return empty list; function's contract is to return a list.
        
    return json.dumps(articles, ensure_ascii=False, indent=2) 


def extract_content(html_content: str) -> dict | None:
    """
    Parses HTML content from an article page to extract its title and main content.

    Args:
        html_content: The HTML content of the article page.

    Returns:
        A dictionary containing 'title' and 'content', or None if parsing fails.
    """
    if not html_content:
        logger.warning("Received empty HTML content for content extraction.")
        return None

    try:
        soup = BeautifulSoup(html_content, 'html.parser')

        title_tag = soup.select_one('h1.article-title.common-width')
        title = title_tag.get_text(strip=True) if title_tag else None

        content_div = soup.select_one('div.articleDetailContent')
        
        if content_div:
            # Use get_text() for more robust content extraction
            content_text = content_div.get_text(separator='\n', strip=True)
        else:
            content_text = None

        if title and content_text:

            return {
                'title': title,
                'content': content_text
            }
        else:
            missing_parts = []
            if not title: missing_parts.append("title (selector: h1.article-title.common-width)")
            if not content_text: missing_parts.append("content (selector: div.articleDetailContent)")
            logger.warning(f"Could not extract full article content. Missing: {', '.join(missing_parts)}")
            if title_tag is None:
                 logger.debug("Title tag 'h1.article-title.common-width' not found.")
            if content_div is None:
                 logger.debug("Content div 'div.articleDetailContent' not found.")
            return None

    except Exception as e:
        logger.error(f"Error during HTML parsing for article content: {e}", exc_info=True)
        return None


def _test_extract_catalogue():
    """Tests the extract_catalogue function."""
    base_url_catalogue = "https://36kr.com"
    html_file_path_catalogue = Path("data/36kr/demo_gateway.html")

    logger.info(f"Attempting to read HTML content from {html_file_path_catalogue}")
    try:
        html_content_catalogue = html_file_path_catalogue.read_text(encoding='utf-8')
        extracted_articles_catalogue = extract_catalogue(html_content_catalogue, base_url_catalogue)

        if extracted_articles_catalogue:
            logger.info(f"Extracted articles (catalogue): \n{extracted_articles_catalogue}")
            try:
                parsed_catalogue = json.loads(extracted_articles_catalogue)
                if isinstance(parsed_catalogue, list) and len(parsed_catalogue) > 0:
                    logger.info(f"Successfully parsed {len(parsed_catalogue)} articles from catalogue.")
                else:
                    logger.warning("Catalogue extraction did not result in a list of articles or the list was empty.")
            except json.JSONDecodeError:
                logger.error("Failed to parse JSON output from extract_catalogue.")
        else:
            logger.warning("No articles were extracted from the catalogue HTML content. Please check selectors and HTML structure.")
    except FileNotFoundError:
        logger.error(f"Catalogue HTML file not found: {html_file_path_catalogue}")
    except Exception as e:
        logger.error(f"An error occurred during catalogue extraction: {e}", exc_info=True)


def _test_extract_content():
    """Tests the extract_content function."""
    html_file_path_content = Path("data/36kr/demo_content.html")

    logger.info(f"Attempting to read HTML content from {html_file_path_content}")
    try:
        html_content_article = html_file_path_content.read_text(encoding='utf-8')
        extracted_main_content = extract_content(html_content_article)

        if extracted_main_content and extracted_main_content['title'] and extracted_main_content['content_text']:
            logger.info(f"Extracted Article Title: {extracted_main_content['title']}")
            logger.info(f"Extracted Article Content (first 200 chars):\n{extracted_main_content['content_text'][:200]}...")

        else:
            logger.warning("No main content (title and/or text) was extracted from the article HTML content. Please check selectors and HTML structure.")
            if extracted_main_content:
                 logger.debug(f"Extraction result: Title present: {bool(extracted_main_content.get('title'))}, Content present: {bool(extracted_main_content.get('content_text'))}")
    except FileNotFoundError:
        logger.error(f"Article content HTML file not found: {html_file_path_content}")
    except Exception as e:
        logger.error(f"An error occurred during article content extraction: {e}", exc_info=True)

def main():
    logger.info("Starting Part 1: Test extraction from demo_gateway.html (catalogue)...")
    _test_extract_catalogue()
    logger.info("\n" + "="*50 + "\n") # Separator
    logger.info("Starting Part 2: Test extraction from demo_content.html (article content)...")
    _test_extract_content()

if __name__ == "__main__":
    main() 