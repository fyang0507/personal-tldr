"""
[GENERATED BY CURSOR]
Script to crawl website content, generate a summary using LLM, and estimate reading time.
Combines functionality from website scraping, LLM summarization, and reading time estimation.
"""

from utils.logging_config import logger
import tomllib
from pathlib import Path
from connectors.llm import api_text_completion
from connectors.website.scrapers import scrape
from utils.read_time_estimate import estimate_read_time


def scrape_and_process_content(url: str, scraper_type="basic"):
    """
    Scrapes a website, generates a summary, and estimates reading time.
    
    Args:
        url: The URL of the website to scrape
        scraper_type: The type of scraper to use ("basic" or "playwright")
        
    Returns:
        dict: Dictionary containing the scraped content, summary, and read time, or None if error
    """
    try:
        logger.info(f"Scraping content from {url} using {scraper_type} scraper")
        
        # Step 1: Scrape website and convert to markdown
        html_content, markdown_content, title = scrape(
            url=url,
            scraper_type=scraper_type
        )
        
        if not markdown_content:
            logger.error(f"Failed to scrape content from {url}")
            return None
        
        # Step 2: Generate summary using LLM
        summary = summarize_content(markdown_content)
        if not summary:
            logger.warning(f"Failed to generate summary for {url}")
        
        # Step 3: Estimate reading time
        read_time = estimate_read_time(markdown_content)
        
        # Return all the processed data
        result = {
            "url": url,
            "title": title,
            "html_content": html_content,
            "markdown_content": markdown_content,
            "summary": summary,
            "read_time": read_time
        }
        
        logger.success(f"Successfully processed content from {url}")
        logger.info(f"Title: {title}")
        logger.info(f"Reading time: {read_time}")
        
        return result
        
    except Exception as e:
        logger.error(f"Error processing content from {url}: {e}")
        return None


def summarize_content(markdown_content):
    """
    Generates a summary of the content using LLM.
    
    Args:
        markdown_content: The markdown content to summarize
        
    Returns:
        str: The summary text, or None if error
    """
    try:
        # Load the prompt from website.toml
        prompt_file_path = Path("prompts/website.toml")
        if not prompt_file_path.exists():
            logger.error(f"Prompt file not found: {prompt_file_path}")
            return None
            
        # Use tomllib to read the TOML file
        with open(prompt_file_path, "rb") as f:
            prompts = tomllib.load(f)
        
        # Get the system prompt and model from the toml file
        system_prompt = prompts['summary']['system']
        model = prompts['summary']['model']
        
        logger.info(f"Generating summary using LLM (model: {model})")
        
        # Use the llm connector to get the response
        response = api_text_completion(
            model=model,
            system_prompt=system_prompt,
            user_message=markdown_content
        )
        
        logger.success(f"Successfully generated summary: {response}")
        return response
    
    except Exception as e:
        logger.error(f"Error generating summary: {e}")
        return None


def save_processed_content(result, output_dir):
    """
    Saves processed content to files.
    
    Args:
        result: Dictionary containing processed content
        output_dir: Directory to save files to
        
    Returns:
        bool: True if successful, False if error
    """
    try:
        # Create clean filename from title
        if result["title"]:
            filename = "".join(c if c.isalnum() or c in [' ', '_', '-'] else '_' for c in result["title"])
            filename = filename.replace(' ', '_')[:50]  # Limit length
        else:
            import hashlib
            filename = hashlib.md5(result["url"].encode()).hexdigest()[:10]
        
        # Ensure output directory exists
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Save markdown content
        markdown_path = output_path / f"{filename}.md"
        with open(markdown_path, "w", encoding="utf-8") as f:
            # Add metadata at the top of the file
            f.write(f"# {result['title']}\n\n")
            f.write(f"URL: {result['url']}\n")
            f.write(f"Reading time: {result['read_time']}\n\n")
            f.write("## Summary\n\n")
            f.write(f"{result['summary']}\n\n")
            f.write("## Content\n\n")
            f.write(result['markdown_content'])
        
        # Save HTML content
        html_path = output_path / f"{filename}.html"
        with open(html_path, "w", encoding="utf-8") as f:
            f.write(result['html_content'])
        
        logger.success(f"Content saved to {markdown_path} and {html_path}")
        return True
        
    except Exception as e:
        logger.error(f"Error saving processed content: {e}")
        return False


def main():
    """Main function with placeholder values."""
    # Target URL to process
    target_url = "https://36kr.com/p/3265228187942663"
    scraper_type = "playwright"
    output_dir = "data/36kr"
    
    # Process the content
    result = scrape_and_process_content(
        url=target_url,
        scraper_type=scraper_type
    )
    
    if result:
        # Save the processed content
        save_processed_content(result, output_dir=output_dir)
        
        # Display summary information
        logger.info(f"Article: {result['title']}")
        logger.info(f"Reading time: {result['read_time']}")
        logger.info(f"Summary: {result['summary'][:150]}...")
    else:
        logger.error(f"Failed to process content from {target_url}")


if __name__ == "__main__":
    main()
