"""
[GENERATED BY CURSOR]
Script to scrape websites with anti-bot protection using Playwright.
Designed to bypass protection mechanisms by using a headless browser.
"""

from pathlib import Path
from utils.logging_config import logger
import random
from .utils import parse_html_to_markdown


async def simulate_browser(url: str):
    """
    Simulates a browser using Playwright to bypass anti-bot protection.
    
    Args:
        url: URL to scrape
        
    Returns:
        tuple: (html_content, page_title) or (None, None) if an error occurs
    """
    try:
        from playwright.async_api import async_playwright
        
        logger.info(f"Starting browser simulation for: {url}")
        
        # Initialize Playwright
        async with async_playwright() as p:
            # Launch browser with common user agent and specific viewport to appear more like a real user
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
                viewport={"width": 1280, "height": 800}
            )
            
            # Add random delays to mimic human behavior
            page = await context.new_page()
            
            # Enable JavaScript
            await page.set_extra_http_headers({
                "Accept-Language": "en-US,en;q=0.9",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1"
            })
            
            # Add cookie consent handling if needed
            await page.route("**/*", lambda route: route.continue_())
            
            # Navigate to the URL with a timeout of 60 seconds
            logger.info(f"Navigating to {url}")
            response = await page.goto(url, wait_until="networkidle", timeout=60000)
            
            if not response:
                logger.error(f"Failed to get response from {url}")
                await browser.close()
                return None, None
                
            if response.status >= 400:
                logger.error(f"Got HTTP error {response.status} for {url}")
                await browser.close()
                return None, None
            
            # Wait for content to load
            # Add random delay to mimic human reading (2-4 seconds)
            await page.wait_for_timeout(random.randint(2000, 4000))
            
            # Scroll down the page to load lazy content
            await page.evaluate("""
                async () => {
                    await new Promise((resolve) => {
                        let totalHeight = 0;
                        const distance = 300;
                        const timer = setInterval(() => {
                            const scrollHeight = document.body.scrollHeight;
                            window.scrollBy(0, distance);
                            totalHeight += distance;
                            
                            if(totalHeight >= scrollHeight){
                                clearInterval(timer);
                                resolve();
                            }
                        }, 100);
                    });
                }
            """)
            
            # Wait a bit after scrolling
            await page.wait_for_timeout(random.randint(1000, 2000))
            
            # Get page content
            html_content = await page.content()
            page_title = await page.title()
            
            logger.info(f"Successfully retrieved content for {url} with title: {page_title}")
            
            # Close browser
            await browser.close()
            
            return html_content, page_title
            
    except ImportError:
        logger.error("Playwright not installed. Install with: pip install playwright")
        logger.error("Then install browsers with: playwright install")
        return None, None
    except Exception as e:
        logger.error(f"Error simulating browser for {url}: {e}")
        return None, None

async def scrape_with_playwright(url: str):
    """
    Scrapes a website using Playwright to bypass anti-bot protection.
    
    Args:
        url: URL to scrape
        
    Returns:
        tuple: (html_content, markdown_content, title) or (None, None, None) if an error occurs
    """
    try:
        # Step 1: Simulate browser to get HTML content
        html_content, page_title = await simulate_browser(url)
        
        if not html_content:
            return None, None, None
            
        # Step 2: Parse HTML to Markdown
        markdown_content, title = parse_html_to_markdown(html_content)
        
        if not markdown_content:
            return html_content, None, page_title
        
        return html_content, markdown_content, title
            
    except Exception as e:
        logger.error(f"Error scraping {url} with Playwright: {e}")
        return None, None, None

def scrape(url: str):
    """
    Synchronous wrapper for the async Playwright scraper.
    
    Args:
        url: URL to scrape
        
    Returns:
        tuple: (html_content, markdown_content, title)
    """
    import asyncio
    
    try:
        # Set up asyncio event loop
        loop = asyncio.get_event_loop()
    except RuntimeError:
        # Create a new event loop if none exists
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    
    result = loop.run_until_complete(scrape_with_playwright(url))
    
    return result

def main():
    """Main function with placeholder values."""
    # Target URL from the user
    target_url = "https://36kr.com/p/3265228187942663"
    output_dir = "data/36kr/playwright"
    
    # Ensure output directory exists
    Path(output_dir).mkdir(parents=True, exist_ok=True)
        
    output_filepath = Path(output_dir) / f"demo_playwright.md"
    html_path = str(output_filepath).replace('.md', '.html')
    
    # Scrape website
    html_content, markdown_content, title = scrape(
        url=target_url
    )
    
    if markdown_content:
        # Save HTML
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        logger.success(f"Saved HTML content to {html_path}")
        
        # Save Markdown
        with open(output_filepath, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        logger.success(f"Saved Markdown content to {output_filepath}")
        
        logger.success(f"Successfully scraped article: {title}")
        logger.info(f"Content saved to {output_dir}")
    else:
        logger.error("Failed to scrape the website.")


if __name__ == "__main__":
    main() 