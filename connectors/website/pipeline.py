"""
[GENERATED BY CURSOR]
Workflow orchestrator for website content processing.
Combines gateway url discovery with content extraction in a complete pipeline.

This module implements a two-phase content retrieval approach:
Phase 1: Check for updates by scraping the gateway page and finding the latest content URL (check_latest_updates)
Phase 2: Process and retrieve the full content details when needed (get_latest_update_details)
"""

from utils.logging_config import logger
import os
from connectors.website import gateway
from connectors.website import content
from typing import Dict, Any, Optional
from utils.connector_cache import ConnectorCache
import tomllib
from functools import lru_cache

@lru_cache(maxsize=None)
def _load_website_configs() -> Dict[str, Any]:
    """Loads website configurations from config/website.toml.
    The result is cached, so the file is read and parsed only once.
    Raises FileNotFoundError or tomllib.TOMLDecodeError if issues occur on first load.
    """
    try:
        with open("config/website.toml", "rb") as f:
            loaded_configs = tomllib.load(f)
        logger.info(f"Successfully loaded website configurations from config/website.toml for {len(loaded_configs)} base URLs (cached).")
        return loaded_configs
    except FileNotFoundError:
        logger.error("Configuration file config/website.toml not found. This error will be cached.")
        raise # Re-raise for the first caller to handle; subsequent calls get cached error or result
    except tomllib.TOMLDecodeError as e:
        logger.error(f"Error decoding config/website.toml: {e}. This error will be cached.")
        raise # Re-raise


def generate_cache_key(channel: str) -> str:
    """
    Generate a standardized cache key for website content.
    
    Args:
        channel: The name of the website channel
        
    Returns:
        A standardized cache key
    """
    # Convert channel name to lowercase and replace spaces with underscores
    normalized_name = channel.lower().replace(' ', '_')
    
    # Return the normalized name as the cache key
    # Note: The date will be added by ConnectorCache
    return normalized_name


def check_latest_updates(
    channel: str, 
    source_url: str, 
    website_config: Dict[str, str],
) -> Optional[Dict[str, Any]]:
    """
    Phase 1: Check for updates from a website source by scraping the gateway page and finding latest content URL.
    Stores the result in cache for later retrieval.
    
    Args:
        channel: Name of the channel
        source_url: URL of the content source (e.g., user profile, publication home)
        website_config: Dictionary containing scraper configurations like 
                        'gateway_scraper', 'gateway_parser', 'content_scraper'.
    Returns:
        Dict containing metadata about the latest content update including URL, or None if error
    """
    # Initialize cache
    cache = ConnectorCache()
    cache_key = generate_cache_key(channel)
    
    logger.info(f"Checking for updates from website: {source_url} for channel '{channel}' using gateway.find_latest_release")
    
    # Call gateway.find_latest_release to get the latest content information
    # This encapsulates scraping the gateway, parsing, and LLM extraction.
    latest_article_info = gateway.find_latest_release(
        url=source_url,
        scraper_type=website_config["gateway_scraper"],
        gateway_parser_name=website_config.get("gateway_parser") # This can be None if generic LLM parsing is intended
    )

    if not latest_article_info:
        logger.warning(f"gateway.find_latest_release returned no information for {source_url}. Channel: '{channel}'.")
        return None
    
    latest_article_url = latest_article_info.get('url')
    # gateway.find_latest_release (via _extract_with_llm) returns 'published_at'
    latest_article_published_at = latest_article_info.get('published_at') 

    if not latest_article_url:
        logger.error(f"Latest article info from gateway.find_latest_release for channel '{channel}' is missing a URL. Data: {latest_article_info}")
        return None
    
    # URL from find_latest_release should already be absolute.
    logger.info(f"Latest content URL identified by gateway.find_latest_release for '{channel}': {latest_article_url}")

    # Prepare result for caching and return
    result = {
        "channel": channel,
        "type": "website",
        "published_at": latest_article_published_at, # Use 'published_at' consistently
        "url": latest_article_url,
        "source_url": source_url,
        "content_scraper": website_config["content_scraper"], # This is for the *next* phase
    }
    
    # Cache the result
    cache.save("website_updates", cache_key, result)
    
    logger.success(f"Successfully found and cached latest content info for {channel}")
    return result


def get_latest_update_details(
    channel: str,
    website_config: Dict[str, str],
) -> Optional[Dict[str, Any]]:
    """
    Phase 2: Process and retrieve the full content details based on cached update information.
    
    Args:
        channel: Name of the website channel
        website_config: Dictionary containing scraper configurations like 
                        'gateway_scraper', 'gateway_parser', 'content_scraper'.
    Returns:
        Dict containing complete metadata and content, or None if error
        
    Raises:
        ValueError: If no update information is found in cache
    """
    try:
        # Initialize cache
        cache = ConnectorCache()
        cache_key = generate_cache_key(channel)
        
        # Try to get latest update information from cache
        latest_update = cache.load("website_updates", cache_key)
        
        if not latest_update or 'url' not in latest_update:
            error_msg = f"No update information found in cache for website channel {channel}"
            logger.error(error_msg)
            raise ValueError(error_msg)
            
        content_url = latest_update['url']
        # Get content_scraper from cache
        logger.info(f"Processing content using {website_config['content_scraper']} scraper for {channel} at URL: {content_url}")
              
        # Step 5: Process the content of the latest URL
        content_result = content.scrape_and_process_content(
            url=content_url,
            scraper_type=website_config["content_scraper"]
        )
        
        if not content_result:
            logger.error(f"Failed to process latest content for {channel}: {content_url}")
            return None
        
        # Combine all results
        result = {
            "title": content_result['title'],
            "channel": channel,
            "type": "website",
            "published_at": latest_update['published_at'],
            "url": content_url,
            "duration": content_result['read_time'],
            "summary": content_result['summary'],
        }
        
        logger.success(f"Successfully retrieved content for {channel}")
        return result
        
    except Exception as e:
        logger.error(f"Error retrieving content for website {channel}: {e}")
        return None


def _prepare_website_processing_config(subscription_config: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """Prepares and validates necessary configurations for website processing.

    Args:
        subscription_config: A dictionary containing channel and source_url.

    Returns:
        A dictionary with 'channel', 'source_url', and 'site_config' if successful, else None.
    """
    # 1. Load all website configurations from config/website.toml
    try:
        all_website_configs = _load_website_configs()
        if not all_website_configs:
            logger.error("Failed to load website configurations or config is empty.")
            return None
    except (FileNotFoundError, tomllib.TOMLDecodeError) as e:
        logger.error(f"Critical error loading config/website.toml: {e}")
        return None

    # 2. Get channel and source_url from the current subscription
    channel = subscription_config.get("channel")
    source_url = subscription_config.get("source_url")

    if not channel or not source_url:
        logger.error(f"Subscription entry is missing 'channel' or 'source_url': {subscription_config}")
        return None

    # 3. Determine base_url
    base_url = gateway.get_base_url(source_url)
    if not base_url:
        logger.error(f"Could not determine base_url for source_url: {source_url} from channel: {channel}")
        return None
    logger.info(f"Determined base_url: {base_url} for source: {source_url}")

    # 4. Get the scraper configuration for this specific base_url
    if base_url not in all_website_configs:
        error_msg = f"No configuration found in config/website.toml for base_url: '{base_url}' (derived from source_url: {source_url} for channel: '{channel}')."
        logger.error(error_msg)
        return None
    
    site_config = all_website_configs[base_url].copy()
    logger.info(f"Loaded scraper parameters for base_url {base_url}: {site_config}")

    # 5. Validate that essential scraper configuration keys are present
    required_keys = ["gateway_scraper", "content_scraper", "gateway_parser"]
    missing_keys = [key for key in required_keys if key not in site_config]

    if missing_keys:
        error_msg = f"Configuration for base_url '{base_url}' in config/website.toml is missing keys: {missing_keys}. (Channel: '{channel}', Source: {source_url})"
        logger.error(error_msg)
        return None

    return {
        "channel": channel,
        "source_url": source_url,
        "site_config": site_config,
        "base_url": base_url # Also pass base_url for logging if needed
    }


def main():
    """Demonstrate the two-phase website content retrieval approach."""
    # Example website channel from subscriptions.toml (conceptually)
    # In a real scenario, this would come from iterating through subscriptions.toml
    subscription_config = {
        "channel": "有数DataVision (36氪)",
        "source_url": "https://36kr.com/user/5294205",
    }

    processing_params = _prepare_website_processing_config(subscription_config)

    if not processing_params:
        logger.error("Failed to prepare website processing configurations. Aborting demo.")
        return

    channel = processing_params["channel"]
    source_url = processing_params["source_url"]
    site_config = processing_params["site_config"]
    base_url = processing_params["base_url"]
    
    logger.info(f"Demonstrating two-phase approach for website channel: {channel}")
    logger.info(f"Using scraper parameters from config/website.toml for {base_url}: {site_config}")
    
    # PHASE 1: Check for updates
    logger.info("\n=== Phase 1: Check for updates ===")
    logger.info("In this phase, we scrape the gateway page to find the latest content URL.")
    latest_update = check_latest_updates(
        channel=channel,
        source_url=source_url,
        website_config=site_config,
    )
    
    if not latest_update:
        logger.error("Failed to find latest content")
        return
        
    logger.info(f"Found latest content URL: {latest_update['url']}")
    logger.info(f"Published: {latest_update['published_at']}")
    logger.info(f"Effective content_scraper for this update: {latest_update['content_scraper']}")
    
    # PHASE 2: Get full content details
    logger.info("\n=== Phase 2: Get full content details ===")
    logger.info("In this phase, we process and retrieve the full content details from the URL in cache.")
    try:
        full_content = get_latest_update_details(
            channel=channel,
            website_config=site_config,
        )
        
        if not full_content:
            logger.error("Failed to retrieve content details")
            return
            
        logger.info(f"Retrieved content: {full_content['title']}")
        logger.info(f"Reading time: {full_content['duration']}")
        logger.info(f"Summary: {full_content['summary'][:150]}...")
    except ValueError as e:
        logger.error(f"Error retrieving content details: {e}")

    logger.success(f"Finished running website pipeline for {source_url}")


if __name__ == "__main__":
    main() 