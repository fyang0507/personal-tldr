"""
[GENERATED BY CURSOR]
Workflow orchestrator for website content processing.
Combines gateway url discovery with content extraction in a complete pipeline.

This module implements a two-phase content retrieval approach:
Phase 1: Check for updates by scraping the gateway page and finding the latest content URL (check_latest_updates)
Phase 2: Process and retrieve the full content details when needed (get_latest_update_details)
"""

from utils.logging_config import logger
from pathlib import Path
import os
from . import gateway
from . import content
from typing import Dict, Any, Optional
from utils.connector_cache import ConnectorCache


def generate_cache_key(channel: str) -> str:
    """
    Generate a standardized cache key for website content.
    
    Args:
        channel: The name of the website channel
        
    Returns:
        A standardized cache key
    """
    # Convert channel name to lowercase and replace spaces with underscores
    normalized_name = channel.lower().replace(' ', '_')
    
    # Return the normalized name as the cache key
    # Note: The date will be added by ConnectorCache
    return normalized_name


def check_latest_updates(
    channel: str, 
    source_url: str, 
    gateway_scraper_type: str = "basic", 
    gateway_content_type: str = "html", 
    output_dir: str = "data/website", 
    debug: bool = False
) -> Optional[Dict[str, Any]]:
    """
    Phase 1: Check for updates from a website source by scraping the gateway page and finding latest content URL.
    Stores the result in cache for later retrieval.
    
    Args:
        channel: Name of the channel
        source_url: URL of the content source (e.g., user profile, publication home)
        gateway_scraper_type: Type of scraper to use for gateway page ("basic" or "playwright")
        gateway_content_type: Content type for gateway analysis ("html" or "markdown")
        output_dir: Base directory to save output files
        debug: If True, save intermediary files for debugging
        
    Returns:
        Dict containing metadata about the latest content update including URL, or None if error
    """
    # Initialize cache
    cache = ConnectorCache()
    cache_key = generate_cache_key(channel)
    
    # Check if we have cached data first
    cached_data = cache.load("website_updates", cache_key)
    if cached_data:
        logger.info(f"Using cached update data for website channel: {channel}")
        return cached_data
    
    try:
        logger.info(f"Checking for updates from website: {source_url}")
        
        # Step 1: Extract base URL
        base_url = gateway.get_base_url(source_url)
        if not base_url:
            logger.error("Failed to extract base URL. Aborting pipeline.")
            return None
        
        # Step 2: Scrape source URL using gateway
        html_content, markdown_content, title = gateway.scrape(
            url=source_url,
            scraper_type=gateway_scraper_type
        )
        
        if not markdown_content:
            logger.error("Failed to scrape source content. Aborting pipeline.")
            return None
        
        # Step 3: Create gateway output directory (only if debug is True)
        if debug:
            gateway_dir = os.path.join(output_dir, "gateway")
            Path(gateway_dir).mkdir(parents=True, exist_ok=True)
            
            # Save markdown content
            markdown_file = os.path.join(gateway_dir, f"debug.md")
            with open(markdown_file, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            # Save HTML content
            html_file = os.path.join(gateway_dir, f"debug.html")
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
        
        # Step 4: Find the latest content URL
        latest_release = gateway.find_latest_release(
            markdown_content=markdown_content,
            html_content=html_content,
            gateway_content_type=gateway_content_type
        )
        
        if not latest_release or 'url' not in latest_release:
            logger.error(f"Failed to find latest content for {channel}: {source_url}")
            return None
            
        # Add base URL if necessary
        if not latest_release['url'].startswith(('http://', 'https://')):
            latest_release['url'] = gateway.urljoin(base_url, latest_release['url'])
            
        logger.info(f"Latest content URL: {latest_release['url']}")
        
        # Return the latest release information including URL
        result = {
            "channel": channel,
            "type": "website",
            "published_at": latest_release['published_at'],
            "url": latest_release['url'],
            "source_url": source_url
        }
        
        # Cache the result
        cache.save("website_updates", cache_key, result)
        
        logger.success(f"Successfully found latest content URL for {channel}")
        return result
        
    except Exception as e:
        logger.error(f"Error checking updates for website {channel}: {e}")
        return None


def get_latest_update_details(
    channel: str,
    content_scraper_type: str = "basic",
    output_dir: str = "data/website",
    debug: bool = False
) -> Optional[Dict[str, Any]]:
    """
    Phase 2: Process and retrieve the full content details based on cached update information.
    
    Args:
        channel: Name of the website channel
        content_scraper_type: Type of scraper to use for content page
        output_dir: Base directory to save output files
        debug: If True, save intermediary files for debugging
        
    Returns:
        Dict containing complete metadata and content, or None if error
        
    Raises:
        ValueError: If no update information is found in cache
    """
    try:
        # Initialize cache
        cache = ConnectorCache()
        cache_key = generate_cache_key(channel)
        
        # Try to get latest update information from cache
        latest_update = cache.load("website_updates", cache_key)
        
        if not latest_update or 'url' not in latest_update:
            error_msg = f"No update information found in cache for website channel {channel}"
            logger.error(error_msg)
            raise ValueError(error_msg)
            
        content_url = latest_update['url']
        logger.info(f"Processing content for {channel} at URL: {content_url}")
              
        # Step 5: Process the content of the latest URL
        content_result = content.scrape_and_process_content(
            url=content_url,
            scraper_type=content_scraper_type
        )
        
        if not content_result:
            logger.error(f"Failed to process latest content for {channel}: {content_url}")
            return None
            
        # Step 6: Save processed content (only if debug is True)
        if debug:
            content_dir = os.path.join(output_dir, "content")
            content.save_processed_content(content_result, output_dir=content_dir)
        
        # Combine all results
        result = {
            "title": content_result['title'],
            "channel": channel,
            "type": "website",
            "published_at": latest_update['published_at'],
            "url": content_url,
            "duration": content_result['read_time'],
            "summary": content_result['summary'],
            "content": content_result.get('content', '')
        }
        
        logger.success(f"Successfully retrieved content for {channel}")
        return result
        
    except Exception as e:
        logger.error(f"Error retrieving content for website {channel}: {e}")
        return None


def main():
    """Demonstrate the two-phase website content retrieval approach."""
    # Example website channel
    channel = "有数DataVision (36氪)"
    source_url = "https://36kr.com/user/5294205"
    gateway_scraper_type = "basic"
    gateway_content_type = "html"
    content_scraper_type = "playwright"
    output_dir = "data/36kr"
    debug = True  # Enable debug mode for development testing
    
    print(f"Demonstrating two-phase approach for website channel: {channel}")
    
    # PHASE 1: Check for updates
    print("\n=== Phase 1: Check for updates ===")
    print("In this phase, we scrape the gateway page to find the latest content URL.")
    latest_update = check_latest_updates(
        channel=channel,
        source_url=source_url,
        gateway_scraper_type=gateway_scraper_type,
        gateway_content_type=gateway_content_type,
        output_dir=output_dir,
        debug=debug
    )
    
    if not latest_update:
        print("Failed to find latest content")
        return
        
    print(f"Found latest content URL: {latest_update['url']}")
    print(f"Published: {latest_update['published_at']}")
    
    # PHASE 2: Get full content details
    print("\n=== Phase 2: Get full content details ===")
    print("In this phase, we process and retrieve the full content details from the URL in cache.")
    try:
        full_content = get_latest_update_details(
            channel=channel,
            content_scraper_type=content_scraper_type,
            output_dir=output_dir,
            debug=debug
        )
        
        if not full_content:
            print("Failed to retrieve content details")
            return
            
        print(f"Retrieved content: {full_content['title']}")
        print(f"Reading time: {full_content['duration']}")
        print(f"Summary: {full_content['summary'][:150]}...")
    except ValueError as e:
        print(f"Error retrieving content details: {e}")


if __name__ == "__main__":
    main() 