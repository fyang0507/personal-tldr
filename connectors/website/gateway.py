"""
[GENERATED BY CURSOR]
Script to scrape a website and save the content to a Markdown file.
Also sends the markdown content to LLM to find the latest articles/newsletter metadata.
"""

from utils.logging_config import logger
import json
import re
import tomllib
from pathlib import Path
from connectors.llm import api_text_completion
from urllib.parse import urlparse, urljoin
from connectors.website.scrapers import scrape

def get_base_url(url: str) -> str:
    """
    Extracts the base URL from a given URL.
    
    Args:
        url: The full URL to extract the base URL from
        
    Returns:
        str: The base URL (scheme + domain)
    """
    try:
        # Parse the URL to extract scheme and netloc (domain)
        parsed_url = urlparse(url)
        base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
        logger.debug(f"Extracted base URL '{base_url}' from '{url}'")
        return base_url
    except Exception as e:
        logger.error(f"Failed to extract base URL from '{url}': {e}")
        # Return the original URL if parsing fails
        return None


def scrape(url: str, scraper_type="basic"):
    """
    Scrapes the content of a given URL using the specified scraper type.

    Args:
        url: The URL of the website to scrape.
        scraper_type: The type of scraper to use ("basic" or "playwright").
        
    Returns:
        tuple: (html_content, markdown_content, title) or (None, None, None) if an error occurs
    """
    try:
        logger.info(f"Attempting to fetch URL using {scraper_type} scraper: {url}")
        
        # Use the scraper from the scrapers module
        from connectors.website.scrapers import scrape as scraper_func
        return scraper_func(url, scraper_type)

    except Exception as e:
        logger.error(f"Failed to convert URL {url} to Markdown using {scraper_type} scraper: {e}")
        return None, None, None


def find_latest_release(markdown_content, html_content, gateway_content_type="html"):
    """
    Sends the content to LLM to find the latest release information.
    
    Args:
        markdown_content: The markdown content to analyze
        html_content: Optional HTML content if needed when content_type is "html"
        gateway_content_type: The type of content to use ("html" or "markdown")
        
    Returns:
        dict: JSON object containing the latest release information
    """
    try:
        # Load the prompt from website.toml
        prompt_file_path = Path("prompts/website.toml")
        if not prompt_file_path.exists():
            logger.error(f"Prompt file not found: {prompt_file_path}")
            return None
            
        # Use tomllib to read the TOML file (Python 3.11+)
        with open(prompt_file_path, "rb") as f:
            prompts = tomllib.load(f)
        
        # Get the system prompt and model from the toml file
        system_prompt = prompts['gateway']['system']
        model = prompts['gateway']['model']
        
        # Determine which content to use based on content_type
        if gateway_content_type.lower() == "html" and html_content:
            logger.info(f"Using HTML content for LLM analysis")
            user_message = html_content
        elif gateway_content_type.lower() == "markdown":
            logger.info(f"Using Markdown content for LLM analysis")
            user_message = markdown_content
        else:
            error_msg = f"Invalid gateway_content_type: {gateway_content_type}. Must be 'html' or 'markdown'"
            logger.error(error_msg)
            raise ValueError(error_msg)
            
        logger.info(f"Sending {gateway_content_type} content to LLM (model: {model})")
        
        # Use the llm connector to get the response
        response = api_text_completion(
            model=model,
            system_prompt=system_prompt,
            user_message=user_message
        )
        
        # Parse the response as JSON
        try:
            # Try to parse directly if response is already in JSON format
            latest_release = json.loads(response)
            logger.success(f"Successfully found latest release: {latest_release}")
            return latest_release
        except json.JSONDecodeError:
            # If not JSON, extract JSON-like content from the response
            logger.warning("Response is not in valid JSON format. Attempting to extract JSON content.")
            
            # Try to find JSON-like structure in the response
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                try:
                    latest_release = json.loads(json_match.group(0))
                    logger.success(f"Successfully extracted JSON from response: {latest_release}")
                    return latest_release
                except json.JSONDecodeError:
                    logger.error("Failed to parse JSON from extracted content")
            
            logger.error(f"Failed to parse JSON from LLM response: {response[:200]}...")
            return {"error": "Failed to parse response", "raw_response": response[:500]}
    
    except Exception as e:
        logger.error(f"Error in find_latest_release: {e}")
        return None


def main():
    """Main function with placeholder values."""
    # Placeholder values (as requested)
    target_url = "https://36kr.com/user/5294210"
    scraper_type = "basic"
    gateway_content_type = "html"
    channel = "远川研究所"
    output_dir = "data/36kr/gateway"
    
    # Create output filenames
    markdown_file = f"{output_dir}/demo.md"
    html_file = f"{output_dir}/demo.html"

    # Step 1: Get base URL
    base_url = get_base_url(target_url)
    if not base_url:
        logger.error("Failed to extract base URL. Aborting.")
        return
    
    # Step 2: Scrape website
    html_content, markdown_content, title = scrape(
        url=target_url, 
        scraper_type=scraper_type,
    )
    
    if not markdown_content:
        logger.error("Failed to scrape website content. Aborting.")
        return
    
    # Step 3: Save both HTML and markdown content to files
    try:
        # Ensure directory exists
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # Save markdown content
        with open(markdown_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        logger.success(f"Saved markdown content to {markdown_file}")
        
        # Save HTML content
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        logger.success(f"Saved HTML content to {html_file}")
        
    except Exception as e:
        logger.error(f"Failed to save content: {e}")
        return
    
    # Step 4: Find latest release information
    latest_release = find_latest_release(
        markdown_content=markdown_content,
        html_content=html_content,
        gateway_content_type=gateway_content_type
    )

    # Step 5: prepend base url to the url in the latest_release
    if latest_release:
        latest_release['url'] = urljoin(base_url, latest_release['url'])
        latest_release['channel'] = channel

        logger.info(f"Latest release information: {json.dumps(latest_release, indent=2, ensure_ascii=False)}")
        logger.success(f"Gateway process complete. Latest release metadata captured.")
    else:
        logger.error("Failed to find latest release information.")


if __name__ == "__main__":
    main() 