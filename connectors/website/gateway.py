"""
[GENERATED BY CURSOR]
Script to scrape a website and save the content to a Markdown file.
Also sends the markdown content to LLM to find the latest articles/newsletter.
"""

from utils.logging_config import logger
import json
import tomllib
from pathlib import Path
from connectors.llm import api_text_completion
from urllib.parse import urlparse, urljoin
from connectors.website.playwright_scrape import scrape_website_to_markdown as playwright_scrape
from connectors.website.basic_scrape import scrape_website_to_markdown as basic_scrape

def get_base_url(url: str) -> str:
    """
    Extracts the base URL from a given URL.
    
    Args:
        url: The full URL to extract the base URL from
        
    Returns:
        str: The base URL (scheme + domain)
    """
    try:
        # Parse the URL to extract scheme and netloc (domain)
        parsed_url = urlparse(url)
        base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
        logger.debug(f"Extracted base URL '{base_url}' from '{url}'")
        return base_url
    except Exception as e:
        logger.error(f"Failed to extract base URL from '{url}': {e}")
        # Return the original URL if parsing fails
        return None


def scrape_website_to_markdown(url: str, gateway_type="basic"):
    """
    Scrapes the content of a given URL using the specified gateway type.

    Args:
        url: The URL of the website to scrape.
        gateway_type: The type of scraper to use ("basic" or "playwright").
        
    Returns:
        tuple: (html_content, markdown_content, title) or (None, None, None) if an error occurs
    """
    try:
        logger.info(f"Attempting to fetch URL using {gateway_type} gateway: {url}")
        
        if gateway_type == "basic":
            # Use the basic scraper implementation
            html_content, markdown_content, title = basic_scrape(url)
        else:
            # Default to Playwright implementation
            html_content, markdown_content, title = playwright_scrape(url)
        
        if not markdown_content:
            logger.error(f"Failed to fetch content from {url}")
            return None, None, None
            
        logger.success(f"Successfully scraped article with title: {title}")
        return html_content, markdown_content, title

    except Exception as e:
        logger.error(f"Failed to convert URL {url} to Markdown using {gateway_type} gateway: {e}")
        return None, None, None


def find_latest_release(markdown_content):
    """
    Sends the markdown content to LLM to find the latest release information.
    
    Args:
        markdown_content: The markdown content to analyze
        
    Returns:
        dict: JSON object containing the latest release information
    """
    try:
        # Load the prompt from website.toml
        prompt_file_path = Path("prompts/website.toml")
        if not prompt_file_path.exists():
            logger.error(f"Prompt file not found: {prompt_file_path}")
            return None
            
        # Use tomllib to read the TOML file (Python 3.11+)
        with open(prompt_file_path, "rb") as f:
            prompts = tomllib.load(f)
        
        # Get the system prompt and model from the toml file
        system_prompt = prompts['gateway']['system']
        model = prompts['gateway']['model']
            
        logger.info(f"Sending markdown content to LLM (model: {model})")
        
        # Use the llm connector to get the response
        response = api_text_completion(
            model=model,
            system_prompt=system_prompt,
            user_message=markdown_content
        )
        
        # Parse the response as JSON
        try:
            # Try to parse directly if response is already in JSON format
            latest_release = json.loads(response)
            logger.success(f"Successfully found latest release: {latest_release}")
            return latest_release
        except json.JSONDecodeError:
            # If not JSON, extract JSON-like content from the response
            logger.warning("Response is not in valid JSON format. Attempting to extract JSON content.")
            
            # Try to find JSON-like structure in the response
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                try:
                    latest_release = json.loads(json_match.group(0))
                    logger.success(f"Successfully extracted JSON from response: {latest_release}")
                    return latest_release
                except json.JSONDecodeError:
                    logger.error("Failed to parse JSON from extracted content")
            
            logger.error(f"Failed to parse JSON from LLM response: {response[:200]}...")
            return {"error": "Failed to parse response", "raw_response": response[:500]}
    
    except Exception as e:
        logger.error(f"Error in find_latest_release: {e}")
        return None


def main():
    """Main function with placeholder values."""
    # Placeholder values (as requested)
    target_url = "https://36kr.com/user/5294210"
    gateway_type = "basic"
    channel = "远川研究所"
    output_dir = "data/36kr/gateway"
    
    # Create filenames based on URL
    domain = urlparse(target_url).netloc
    path = urlparse(target_url).path.replace('/', '_')
    if path.startswith('_'):
        path = path[1:]
    if path.endswith('_'):
        path = path[:-1]
    
    # Limit filename length
    if len(path) > 50:
        path = path[:50]
    
    # Create output filenames
    markdown_file = f"{output_dir}/{domain}_{path}.md"
    html_file = f"{output_dir}/{domain}_{path}.html"

    # Step 1: Get base URL
    base_url = get_base_url(target_url)
    if not base_url:
        logger.error("Failed to extract base URL. Aborting.")
        return
    
    # Step 2: Scrape website
    html_content, markdown_content, title = scrape_website_to_markdown(
        url=target_url, 
        gateway_type=gateway_type
    )
    
    if not markdown_content:
        logger.error("Failed to scrape website content. Aborting.")
        return
    
    # Step 3: Save both HTML and markdown content to files
    try:
        # Ensure directory exists
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # Save markdown content
        with open(markdown_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        logger.success(f"Saved markdown content to {markdown_file}")
        
        # Save HTML content
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        logger.success(f"Saved HTML content to {html_file}")
        
    except Exception as e:
        logger.error(f"Failed to save content: {e}")
        return
    
    # Step 4: Find latest release information
    latest_release = find_latest_release(markdown_content)
    
    if latest_release:
        logger.info(f"Latest release information: {json.dumps(latest_release, indent=2, ensure_ascii=False)}")
        logger.success(f"Found latest release URL: {latest_release.get('url', 'N/A')}")
        logger.success(f"Gateway process complete. Latest release metadata captured.")
    else:
        logger.error("Failed to find latest release information.")


if __name__ == "__main__":
    main() 