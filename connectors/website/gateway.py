"""
[GENERATED BY CURSOR]
Script to scrape a website and save the content to a Markdown file.
Also sends the markdown content to LLM to find the latest articles/newsletter metadata.
"""

from datetime import datetime
from utils.logging_config import logger
import json
import tomllib
from pathlib import Path
from connectors.llm import api_text_completion
from urllib.parse import urlparse, urljoin
from connectors.website.scrapers import scrape
from typing import Optional, Dict, Any
import importlib
import types

def get_base_url(url: str) -> str:
    """
    Extracts the base URL from a given URL.
    
    Args:
        url: The full URL to extract the base URL from
        
    Returns:
        str: The base URL (scheme + domain)
    """
    try:
        # Parse the URL to extract scheme and netloc (domain)
        parsed_url = urlparse(url)
        base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
        logger.debug(f"Extracted base URL '{base_url}' from '{url}'")
        return base_url
    except Exception as e:
        logger.error(f"Failed to extract base URL from '{url}': {e}")
        # Return the original URL if parsing fails
        return None


def scrape(url: str, scraper_type="basic"):
    """
    Scrapes the content of a given URL using the specified scraper type.

    Args:
        url: The URL of the website to scrape.
        scraper_type: The type of scraper to use ("basic" or "playwright").
        
    Returns:
        html content or None if an error occurs
    """
    try:
        logger.info(f"Attempting to fetch URL using {scraper_type} scraper: {url}")
        
        # Use the scraper from the scrapers module
        from connectors.website.scrapers import scrape as scraper_func
        return scraper_func(url, scraper_type)

    except Exception as e:
        logger.error(f"Failed to scrape URL {url} using {scraper_type} scraper: {e}")
        return None


def load_gateway_parser(gateway_parser_name: str) -> Optional[types.ModuleType]:
    """
    Dynamically imports a parser module from the connectors.website.parsers package.
    Args:
        gateway_parser_name: The name of the parser module (e.g., "parser_36kr").
    Returns:
        The loaded module object, or None if an error occurs.
    """
    if not gateway_parser_name:
        logger.error("Parser module name cannot be empty for dynamic loading.")
        return None
    try:
        module_path = f"connectors.website.parsers.{gateway_parser_name}"
        module = importlib.import_module(module_path)
        logger.info(f"Successfully loaded parser module: {module_path}")
        return module
    except ImportError:
        logger.warning(f"Parser module '{module_path}' not found or caused an ImportError.")
        return None # Return None on ImportError so fallback can be triggered
    except Exception as e:
        logger.error(f"An unexpected error occurred while loading parser module '{gateway_parser_name}': {e}", exc_info=True)
        return None


def _extract_with_llm(parsing_results: str, base_url: str) -> Optional[Dict[str, Any]]:
    """
    Uses LLM to find the latest release from the processed data.
    """
    logger.info("Finding latest release...")

    prompt_file_path = Path("prompts/website.toml")
    if not prompt_file_path.exists():
        logger.error(f"Prompt file not found: {prompt_file_path}")
        return None
        
    with open(prompt_file_path, "rb") as f:
        prompts = tomllib.load(f)
    
    system_prompt = prompts['gateway']['system'].format(today=datetime.now().strftime('%Y-%m-%d'))
    model = prompts['gateway']['model']
        
    response = api_text_completion(
        model=model,
        system_prompt=system_prompt,
        user_message=parsing_results
    )
    
    try:
        llm_result = json.loads(response)
        if llm_result.get('published_at') == 'N/A':
            llm_result['published_at'] = datetime.now().strftime('%Y-%m-%d')
        
        # Ensure URL from LLM is absolute
        if 'url' in llm_result and not urlparse(llm_result['url']).scheme:
            llm_result['url'] = urljoin(base_url, llm_result['url'])
        
        return llm_result
    
    except json.JSONDecodeError:
        logger.error(f"Failed to parse JSON from LLM response: {response[:200]}...")
        return None # Changed from raising error to returning None for pipeline flow
    except Exception as e:
        logger.error(f"Error processing LLM response: {e}", exc_info=True)
        return None


def find_latest_release(url: str, scraper_type: str, gateway_parser_name: Optional[str] = None) -> Optional[Dict[str, Any]]:
    """
    Orchestrates fetching, parsing (specific or generic), and finding the latest release from a gateway URL.
    """
    base_url = get_base_url(url)
    if not base_url:
        # Error logged by get_base_url
        return None

    # Step 1: Scrape the website
    html_content = scrape(url, scraper_type)
    if not html_content:
        # Error logged by scrape
        return None

    # Step 2: Dynamically load the parser module and parse the html content
    if gateway_parser_name:
        logger.info(f"Attempting to use specific parser '{gateway_parser_name}' for {url}.")
        parser_mod = load_gateway_parser(gateway_parser_name)
        if parser_mod:
            try:
                extract_catalogue = getattr(parser_mod, "extract_catalogue")
                logger.info(f"Using '{gateway_parser_name}.extract_catalogue' for {url}.")
                parsing_results = extract_catalogue(html_content, base_url)
            except AttributeError:
                logger.warning(f"Parser module '{gateway_parser_name}' does not have an 'extract_catalogue' function. Will fallback.")
            except Exception as e:
                logger.error(f"Error calling '{gateway_parser_name}.extract_catalogue': {e}", exc_info=True)
    else:
        logger.info(f"No specific gateway_parser_name provided for {url}. Will use generic parser.")
        parser_mod = load_gateway_parser('generic')
        extract_catalogue = getattr(parser_mod, "extract_catalogue")
        parsing_results = extract_catalogue(html_content)

    # Step 3: Use LLM to find the latest release from the parsing results
    latest_release_info = _extract_with_llm(parsing_results, base_url)

    return latest_release_info


def main():
    """Main function with placeholder values."""
    
    # For main, we need to simulate gateway_parser_name that pipeline would get from config
    latest_release_36kr = find_latest_release(
        url="https://36kr.com/user/5294210",
        scraper_type="playwright",
        # gateway_parser_name="36kr",
    )
    if latest_release_36kr:
        logger.info(f"36kr - Latest release: {json.dumps(latest_release_36kr, indent=2, ensure_ascii=False)}")
    else:
        logger.error("36kr - Failed to find latest release information.")


if __name__ == "__main__":
    main() 