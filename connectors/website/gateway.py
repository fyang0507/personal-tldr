"""
[GENERATED BY CURSOR]
Script to scrape a website and parse the content to find the latest articles/newsletter metadata.
The specific method to scrape and parse is specified in the `scraper_type` and `parser_name` arguments.

find_latest_release is the main function to use. It takes a URL, a scraper type, and a parser name.
It returns a dictionary with the latest articles/newsletter metadata with keys: title, url, published_at.
"""

from datetime import datetime
from utils.logging_config import logger
import json
import tomllib
from pathlib import Path
from connectors.llm import api_text_completion
from urllib.parse import urlparse, urljoin
from connectors.website.scrapers import get_scraper
from connectors.website.parsers.base import BaseParser
from connectors.website.parsers import get_parser
from typing import Optional, Dict, Any
from utils.url_utils import extract_base_url


def scrape_html(url: str, scraper_type="basic") -> Optional[str]:
    """
    Scrapes the content of a given URL using the specified scraper type.
    Returns HTML content.
    """
    try:
        logger.info(f"Attempting to fetch URL using {scraper_type} scraper: {url}")
        scraper = get_scraper(scraper_type)
        if scraper:
            return scraper.scrape(url)
        else:
            return None
    except Exception as e:
        logger.error(f"Failed to scrape URL {url} using {scraper_type} scraper: {e}", exc_info=True)
        return None


def _extract_with_llm(parsing_results: Optional[Any], base_url: str) -> Optional[Dict[str, Any]]:
    """
    Uses LLM to find the latest release from the processed data.
    Ensures parsing_results is stringified for the LLM.
    """
    if parsing_results is None:
        logger.warning("Cannot extract with LLM: parsing_results is None.")
        return None

    logger.info("Finding latest release using LLM...")

    if not isinstance(parsing_results, str):
        logger.warning(f"LLM input (parsing_results from catalogue) is not a string (type: {type(parsing_results)}). Converting to JSON string. Data snippet: {str(parsing_results)[:200]}")
        try:
            parsing_results_str = json.dumps(parsing_results, ensure_ascii=False)
        except TypeError as te:
            logger.error(f"Could not convert parsing_results to JSON string for LLM due to TypeError: {te}. Using str() as fallback. Original type: {type(parsing_results)}")
            parsing_results_str = str(parsing_results) 
    else:
        parsing_results_str = parsing_results

    prompt_file_path = Path("prompts/website.toml")
    if not prompt_file_path.exists():
        logger.error(f"Prompt file not found: {prompt_file_path}")
        return None
        
    with open(prompt_file_path, "rb") as f:
        prompts = tomllib.load(f)
    
    system_prompt = prompts['gateway']['system'].format(today=datetime.now().strftime('%Y-%m-%d'))
    model = prompts['gateway']['model']
        
    response = api_text_completion(
        model=model,
        system_prompt=system_prompt,
        user_message=parsing_results_str
    )
    
    try:
        llm_result = json.loads(response)
        if llm_result.get('published_at') == 'N/A':
            llm_result['published_at'] = datetime.now().strftime('%Y-%m-%d')
        
        if 'url' in llm_result and not urlparse(llm_result['url']).scheme and base_url:
            llm_result['url'] = urljoin(base_url, llm_result['url'])
        elif 'url' in llm_result and not urlparse(llm_result['url']).scheme and not base_url:
            logger.warning(f"LLM returned relative URL '{llm_result['url']}' but base_url is None. URL may be incomplete.")
        
        return llm_result
    
    except json.JSONDecodeError:
        logger.error(f"Failed to parse JSON from LLM response: {response[:200]}...")
        return None
    except Exception as e:
        logger.error(f"Error processing LLM response: {e}", exc_info=True)
        return None


def parse_catalogue(html_content: str, base_url: Optional[str], gateway_parser_name: Optional[str]) -> Optional[Any]:
    """
    Parses HTML content to extract a catalogue of items using the specified or generic parser.
    """
    parser: Optional[BaseParser] = None
    parsing_results: Optional[Any] = None

    effective_parser_name = gateway_parser_name if gateway_parser_name else "generic"

    logger.info(f"Attempting to use parser '{effective_parser_name}' for catalogue extraction.")
    parser = get_parser(effective_parser_name)

    if parser:
        logger.info(f"Using '{effective_parser_name}' parser instance's extract_catalogue method.")
        parsing_results = parser.extract_catalogue(html_content, base_url=base_url)

        if parsing_results is None:
            logger.warning(f"Parser '{effective_parser_name}' returned None for catalogue.")
    else:
        logger.error(f"Could not get parser instance for '{effective_parser_name}'. Cannot parse catalogue.")
        # Explicitly return None here, as parser could not be obtained.
        return None
    
    return parsing_results


def find_latest_release(url: str, scraper_type: str, gateway_parser_name: Optional[str] = None) -> Optional[Dict[str, Any]]:
    """
    Orchestrates fetching, parsing (specific or generic), and finding the latest release from a gateway URL.
    """
    base_url = extract_base_url(url)
    if not base_url:
        logger.warning(f"Could not determine base_url for {url} using extract_base_url. Relative URLs from LLM might be incomplete.")

    html_content = scrape_html(url, scraper_type)
    if not html_content:
        return None

    parsing_results = parse_catalogue(html_content, base_url, gateway_parser_name)
    
    if parsing_results is None:
        logger.warning(f"Catalogue parsing results are None for {url} after attempting with parser. Cannot proceed to LLM extraction for latest release.")
        return None
        
    latest_release_info = _extract_with_llm(parsing_results, base_url)
    return latest_release_info


def main():
    """Main function with placeholder values."""
    logger.info("Attempting to find latest release for 36kr user (generic parser if specific fails or not given)...")
    latest_release = find_latest_release(
        url="https://36kr.com/user/5294210",
        scraper_type="playwright", 
        gateway_parser_name="36kr"
    )
    if latest_release:
        logger.info(f"36kr User - Latest release: {json.dumps(latest_release, indent=2, ensure_ascii=False)}")
    else:
        logger.error("36kr User - Failed to find latest release information.")


if __name__ == "__main__":
    main() 