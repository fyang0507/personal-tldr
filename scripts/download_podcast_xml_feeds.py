"""
[GENERATED BY CURSOR]
Script to download XML feeds for podcasts listed in subscriptions.toml.

This script:
1. Reads podcast names from subscriptions.toml
2. Uses PodcastConnector._get_podcast_feed_url to find the feed URL
3. Downloads the XML content for each feed
4. Saves the XML files to data/podcast_xml_feeds/ directory
5. Creates a summary report of the results

These XML files can be used as test cases for LLM-based podcast data extraction.
"""

import sys
import requests
import tomllib
from pathlib import Path
from datetime import datetime
import asyncio
from loguru import logger

# Add the project root to the Python path so we can import the necessary modules
sys.path.append(str(Path(__file__).parent.parent))

# Import the PodcastConnector from the project
from connectors.sources.podcast import PodcastConnector

# Output directory
OUTPUT_DIR = Path(__file__).parent.parent / "data" / "podcast_xml_feeds"

def sanitize_filename(podcast_name: str) -> str:
    """Create a valid filename from a podcast name."""
    # Remove characters that are problematic in filenames
    sanitized = podcast_name.replace('/', '_').replace('\\', '_')
    sanitized = sanitized.replace(':', '_').replace('*', '_')
    sanitized = sanitized.replace('?', '_').replace('"', '_')
    sanitized = sanitized.replace('<', '_').replace('>', '_')
    sanitized = sanitized.replace('|', '_').replace(' ', '_')
    return sanitized

def load_podcast_names() -> list[str]:
    """Load podcast names from subscriptions.toml."""
    try:
        config_path = Path(__file__).parent.parent / "subscriptions.toml"
        with open(config_path, "rb") as f:
            config = tomllib.load(f)
        
        # The 'podcast' key in subscriptions.toml should contain a list of podcast names
        if "podcast" not in config or not isinstance(config["podcast"], list):
            logger.error("No podcast list found in subscriptions.toml or invalid format")
            return []
        
        return config["podcast"]
    except Exception as e:
        logger.error(f"Error loading podcast names: {e}")
        return []

async def download_podcast_feed(podcast_name: str) -> bool:
    """
    Download the XML feed for a single podcast and save it to the output directory.
    
    Returns:
        bool: True if successfully downloaded, False otherwise
    """
    logger.info(f"Processing podcast: {podcast_name}")
    
    # Create a PodcastConnector instance to get the feed URL
    connector = PodcastConnector(podcast_name)
    feed_url = connector._get_podcast_feed_url()
    
    if not feed_url:
        logger.error(f"Failed to get feed URL for podcast: {podcast_name}")
        return False
    
    logger.info(f"Found feed URL: {feed_url}")
    
    try:
        # Download the XML content
        response = requests.get(feed_url, timeout=30)
        response.raise_for_status()
        
        # Create a filename based on the podcast name
        filename = sanitize_filename(podcast_name) + ".xml"
        output_path = OUTPUT_DIR / filename
        
        # Save the XML content
        with open(output_path, "wb") as f:
            f.write(response.content)
            
        logger.success(f"Successfully saved XML feed for '{podcast_name}' to {output_path}")
        return True
    except requests.RequestException as e:
        logger.error(f"Error downloading feed for '{podcast_name}': {e}")
        return False
    except Exception as e:
        logger.error(f"Unexpected error processing '{podcast_name}': {e}")
        return False

async def main():
    """Main entry point for the script."""
    # Create output directory if it doesn't exist
    OUTPUT_DIR.mkdir(exist_ok=True, parents=True)
    
    logger.info(f"Starting podcast XML feed download process")
    logger.info(f"Output directory: {OUTPUT_DIR}")
    
    # Load podcast names
    podcast_names = load_podcast_names()
    logger.info(f"Found {len(podcast_names)} podcasts in subscriptions.toml")
    
    if not podcast_names:
        logger.warning("No podcasts to process, exiting")
        return
    
    # Process each podcast
    results = []
    for podcast_name in podcast_names:
        success = await download_podcast_feed(podcast_name)
        results.append((podcast_name, success))
    
    # Generate summary
    successful = [name for name, success in results if success]
    failed = [name for name, success in results if not success]
    
    logger.info(f"XML Download Summary:")
    logger.info(f"  Total podcasts: {len(podcast_names)}")
    logger.info(f"  Successfully downloaded: {len(successful)}")
    logger.info(f"  Failed to download: {len(failed)}")
    
    if failed:
        logger.warning("The following podcasts failed to download:")
        for name in failed:
            logger.warning(f"  - {name}")
    
    # Create a simple metadata file with the download timestamp and stats
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    metadata_path = OUTPUT_DIR / "download_metadata.txt"
    with open(metadata_path, "w") as f:
        f.write(f"Downloaded on: {timestamp}\n")
        f.write(f"Total podcasts: {len(podcast_names)}\n")
        f.write(f"Successfully downloaded: {len(successful)}\n")
        f.write(f"Failed: {len(failed)}\n\n")
        f.write("Successful downloads:\n")
        for name in successful:
            f.write(f"  - {name}\n")
        if failed:
            f.write("\nFailed downloads:\n")
            for name in failed:
                f.write(f"  - {name}\n")

if __name__ == "__main__":
    # Set up logging
    logger.remove()  # Remove default handler
    logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>")
    logger.add(OUTPUT_DIR / "download_log.txt", rotation="10 MB")
    
    asyncio.run(main()) 