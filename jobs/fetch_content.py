"""
[GENERATED BY CURSOR]
This script implements the second phase of the two-phase content retrieval approach:
1. Reads content requests generated by filter.py (content_request_[date].json)
2. For each content request, fetches full content details using connector-specific get_latest_update_details
3. Outputs complete content data to raw_results_[date].json for preprocessing

This approach minimizes API usage by only fetching content for filtered items
that passed the first phase check.
"""

import json
import os
from datetime import datetime
from utils.logging_config import logger

# Import connector functions for getting full content details
from connectors.youtube import get_latest_update_details as youtube_get_details
from connectors.podcast import get_latest_update_details as podcast_get_details
from connectors.bilibili import get_latest_update_details as bilibili_get_details
from connectors.website.pipeline import get_latest_update_details as website_get_details
from connectors.website.pipeline import get_validated_website_config

def fetch_content_by_type(content_request):
    """
    Fetch full content details by calling the appropriate connector function.
    
    Args:
        content_request (dict): The content request metadata
        
    Returns:
        dict: Full content details or None if retrieval fails
    """
    content_type = content_request.get('type')
    channel = content_request.get('channel', 'Unknown channel')
    
    logger.info(f"Fetching content for {content_type}:{channel}")
    
    try:
        # Call the appropriate get_latest_update_details function based on content type
        if content_type == 'youtube':
            return youtube_get_details(channel)
            
        elif content_type == 'podcast':
            return podcast_get_details(channel)
            
        elif content_type == 'bilibili':
            # For bilibili, we need the UID
            uid = content_request.get('uid')
            if not uid and 'aid' in content_request:
                # If UID is not directly available but we have aid (from the example),
                # we can use channel name only as it should be cached already
                return bilibili_get_details(channel)
            elif not uid:
                logger.error(f"Missing UID for Bilibili channel: {channel}")
                return None
            return bilibili_get_details(channel)
            
        elif content_type == 'website':
            source_url = content_request.get('source_url')
            # 'channel' is already extracted earlier in this function.

            if not source_url:
                logger.error(f"Missing 'source_url' in content_request for website channel: {channel}. Cannot fetch content.")
                return None
            
            try:
                # Use the new centralized function to get validated config
                scraper_params_for_site = get_validated_website_config(source_url=source_url, channel=channel)
                logger.info(f"Using validated scraper config for {channel} (fetch phase): {scraper_params_for_site}")

                return website_get_details(
                    channel=channel,
                    website_config=scraper_params_for_site,
                )
            except ValueError as e:
                # Errors from get_validated_website_config (e.g., config not found, incomplete) are caught here
                logger.error(f"Cannot fetch content for website '{channel}' (source: {source_url}) due to configuration error: {e}")
                return None
            # Other unexpected errors during website_get_details will be caught by the broader try-except in the main calling loop

        else:
            logger.error(f"Unsupported content type: {content_type}")
            return None
            
    except Exception as e:
        logger.error(f"Error fetching content for {content_type}:{channel}: {e}")
        return None


def main():
    """Main function to fetch full content for all content requests."""

    # Define specific content request file path
    # Get today's date in the format YYYY-MM-DD
    today = datetime.now().strftime("%Y-%m-%d")
    content_request_path = f"data/content_request_{today}.json"
    
    # Get today's date for the output file
    today = datetime.now().strftime("%Y-%m-%d")
    raw_results_path = f"data/raw_results_{today}.json"
    
    logger.info("=" * 80)
    logger.info("STARTING PHASE 2: DETAILED CONTENT RETRIEVAL")
    logger.info("=" * 80)
    
    # Check if content request file exists
    if not os.path.exists(content_request_path):
        logger.error(f"Error: {content_request_path} does not exist.")
        return
        
    # Load content requests
    with open(content_request_path, 'r', encoding='utf-8') as f:
        content_requests = json.load(f)
        
    if not content_requests:
        logger.info("No content requests found. Nothing to fetch.")
        # Create an empty raw_results file to allow the pipeline to continue
        with open(raw_results_path, 'w', encoding='utf-8') as f:
            json.dump([], f, ensure_ascii=False, indent=2)
        return
        
    logger.info(f"Found {len(content_requests)} content requests to process")
    
    # Process each content request
    raw_results = []
    success_count = 0
    failure_count = 0
    
    for index, request in enumerate(content_requests, 1):
        channel = request.get('channel', 'Unknown channel')
        content_type = request.get('type', 'Unknown type')
        
        logger.info(f"Fetching content {index}/{len(content_requests)}: {channel} - {content_type}")
        
        # Fetch full content details
        content_details = fetch_content_by_type(request)
        
        if content_details:
            raw_results.append(content_details)
            success_count += 1
            logger.info(f"Successfully fetched content for {channel} - {content_type}")
        else:
            failure_count += 1
            logger.warning(f"Failed to fetch content for {channel} - {content_type}")
            
    # Save raw results
    with open(raw_results_path, 'w', encoding='utf-8') as f:
        json.dump(raw_results, f, ensure_ascii=False, indent=2)
        
    logger.info("=" * 80)
    logger.info(f"PHASE 2 RESULTS: Successfully fetched {success_count}/{len(content_requests)} content items")
    
    if failure_count > 0:
        logger.warning(f"Failed to fetch {failure_count} items")
        
    logger.info(f"Raw results saved to {raw_results_path}")
    logger.info(f"Next step: Run preprocess.py to process the raw results")
    logger.info("=" * 80)


if __name__ == "__main__":
    main() 