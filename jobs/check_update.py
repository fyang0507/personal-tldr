"""
[GENERATED BY CURSOR]
This script retrieves the latest updates from various content sources
listed in subscriptions.toml, checks for new content, and caches update information.

This implements the first phase of the two-phase content retrieval approach:
1. Check for updates and cache minimal metadata (this script)
2. Retrieve full content details when needed (implemented in jobs/preprocess.py)
"""

from dotenv import load_dotenv
from utils.logging_config import logger
from connectors.sources.youtube import YoutubeConnector
from connectors.sources.podcast import PodcastConnector
from connectors.sources.bilibili import BilibiliConnector
from connectors.sources.website.pipeline import WebsiteConnector
import json
from datetime import datetime
import pathlib
from utils.toml_loader import load_toml_file
from utils.connector_cache import ConnectorCache
import asyncio

def load_environment():
    """
    Load and validate all required environment variables.

    Returns:
        dict: Dictionary containing required environment variables for retrieval.

    Raises:
        ValueError: If any required environment variable is missing.
    """
    load_dotenv()

    required_env_vars = {
        # YOUTUBE_API_KEY is no longer directly needed by this script for YouTube
        # but other connectors might still need env_vars, so we keep the structure.
        # 'YOUTUBE_API_KEY': os.getenv('YOUTUBE_API_KEY') 
    }
    # If other connectors add their specific keys here, the logic remains valid.
    # For now, if only YouTube was using it, this dict might be empty or we might adjust
    # the error checking if no vars are strictly required at this level anymore.
    # For safety, let's assume other env vars might be needed by other connectors.
    # So, we just remove YOUTUBE_API_KEY from direct check here.
    
    # Let's refine env_vars to only include what's explicitly fetched and needed by other sync connectors
    # If no other connector needs specific env_vars from this function, it can be simplified.
    # For now, keeping it flexible.
    
    # Example: If PodcastConnector needed a key, it would be:
    # 'PODCAST_API_KEY': os.getenv('PODCAST_API_KEY')
    # And then validated.
    
    # For this iteration, let's assume no other connectors need env vars passed from here.
    # So, the required_env_vars can be empty if YOUTUBE_API_KEY was the only one.
    # However, load_dotenv() is still important for YoutubeConnector itself.

    # Re-evaluating load_environment: YoutubeConnector loads its own .env.
    # This function was primarily for YOUTUBE_API_KEY for the old functional connector.
    # Let's simplify load_environment if no other connector depends on it passing env_vars.
    # For now, just ensure load_dotenv() is called, which it is.
    # The return value `env_vars` is not used for Youtube,
    # but might be for other *synchronous* connectors if they were to expect it.

    return {} # Return empty dict if no other env vars are needed by other connectors from here.


async def process_youtube_channels(youtube_channels):
    """Check for updates from YouTube channels and return results."""
    results = []
    # youtube_api_key is no longer needed here, YoutubeConnector handles it.

    for channel_name in youtube_channels:
        logger.info(f"Checking for updates from YouTube channel: {channel_name}")
        try:
            # YoutubeConnector takes channel and optional duration_min. API key is handled internally.
            youtube_connector = YoutubeConnector(channel=channel_name) 
            
            await youtube_connector.check_latest_updates() # Populates cache, returns None
            
            # Now, get the details that were just cached.
            update_info = await youtube_connector.get_latest_update_details() 
            
            if update_info:
                # Ensure 'type' and 'channel' are in update_info if not already added by connector
                if 'type' not in update_info: update_info['type'] = 'youtube'
                if 'channel' not in update_info: update_info['channel'] = channel_name
                results.append(update_info)
                logger.info(f"Successfully processed and retrieved update info for YouTube channel: {channel_name}")
            else:
                logger.warning(f"No updates found or failed to retrieve details for YouTube channel: {channel_name} after checking.")
        except ValueError as e: # Catch init errors like missing API key
            logger.error(f"Failed to initialize YoutubeConnector for {channel_name}: {e}")
        except Exception as e:
            logger.error(f"Error processing YouTube channel {channel_name}: {e}", exc_info=True)
            
    return results


async def process_podcasts(podcast_names):
    """Check for updates from podcasts and return results."""
    results = []
    # podcast_connector = PodcastConnector() 
    for podcast_name in podcast_names:
        logger.info(f"Checking for updates from podcast: {podcast_name}")
        try:
            podcast_connector = PodcastConnector(podcast_name) 
            update_info = await podcast_connector.check_latest_updates() 
            if update_info:
                results.append(update_info)
            else:
                logger.warning(f"No updates found for podcast: {podcast_name}")
        except Exception as e:
            logger.error(f"Error processing Podcast {podcast_name}: {e}", exc_info=True)
    return results


async def process_bilibili(bilibili_users):
    """Check for updates from Bilibili users and return results."""
    results = []
    
    if not bilibili_users:
        logger.info("No Bilibili users found in subscriptions.")
        return results
            
    for user in bilibili_users:
        uid = user.get('uid')
        name = user.get('name')
        
        if not uid:
            logger.error(f"Missing uid for Bilibili user: {name}")
            continue
            
        logger.info(f"Checking for updates from Bilibili user: {name} (uid: {uid})")
        try:
            bilibili_connector = BilibiliConnector(uid=uid, channel=name) 
            update_info = await bilibili_connector.check_latest_updates() 
            if update_info:
                results.append(update_info)
            else:
                logger.warning(f"No updates found for Bilibili user: {name}")
        except Exception as e:
            logger.error(f"Error processing Bilibili user {name}: {e}", exc_info=True)
            
        await asyncio.sleep(2) # Keep the delay, but make it non-blocking
            
    return results


async def process_websites(websites):
    """Check for updates from websites and return results."""
    results = []
    
    for website_subscription in websites:
        channel = website_subscription.get('channel')
        source_url = website_subscription.get('source_url')
        
        if not channel or not source_url:
            logger.error(f"Missing channel or source_url in website subscription: {website_subscription}. Skipping.")
            continue
            
        logger.info(f"Processing website subscription: {channel} ({source_url})")
        
        try:
            website_connector = WebsiteConnector(channel=channel, source_url=source_url) 
            update_info = await website_connector.check_latest_updates() 
            
            if update_info:
                if 'source_url' not in update_info: # Keep this safety check
                    update_info['source_url'] = source_url
                results.append(update_info)
            else:
                logger.warning(f"No updates found for website: {channel}")

        except ValueError as e:
            logger.error(f"Skipping website {channel} due to configuration error: {e}")
        except Exception as e:
            logger.error(f"Error during website_check_updates for {channel} ({source_url}): {e}. Skipping.")
            
    return results


async def main():
    """Main function to check for updates from all sources and save results."""
    try:
        _ = load_environment() # Call to ensure dotenv is loaded, ignore return if not used by sync connectors

        cache = ConnectorCache()
        cache_dir = cache.cache_dir
        
        if cache_dir.exists():
            logger.info(f"Clearing all caches from: {cache_dir}")
            for cache_file in cache_dir.glob("*.json"):
                try:
                    cache_file.unlink()
                    logger.debug(f"Removed cache file: {cache_file.name}")
                except Exception as e:
                    logger.error(f"Error removing cache file {cache_file.name}: {e}")
        
        subscriptions = load_toml_file("subscriptions.toml")

        # Schedule all processing tasks to run concurrently
        logger.info("Scheduling update checks for all sources...")
        youtube_task = process_youtube_channels(subscriptions.get('youtube', []))
        podcast_task = process_podcasts(subscriptions.get('podcast', []))
        bilibili_task = process_bilibili(subscriptions.get('bilibili', []))
        website_task = process_websites(subscriptions.get('website', []))

        logger.info("Starting concurrent update checks...")
        results_from_all_sources = await asyncio.gather(
            youtube_task,
            podcast_task,
            bilibili_task,
            website_task
        )
        logger.info("Concurrent update checks completed.")

        # Unpack results
        youtube_results, podcast_results, bilibili_results, website_results = results_from_all_sources
        
        all_update_results = youtube_results + podcast_results + bilibili_results + website_results

        for result in all_update_results:
            for key, value in result.items():
                if isinstance(value, datetime):
                    result[key] = value.isoformat()

        if not all_update_results:
            logger.warning("No updates found from any source.")
            return

        data_dir = pathlib.Path("data")
        data_dir.mkdir(exist_ok=True)
        current_date_str = datetime.now().strftime("%Y-%m-%d")
        results_file = data_dir / f"update_check_results_{current_date_str}.json"

        logger.info(f"Saving update check results to: {results_file}")
        try:
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(all_update_results, f, indent=2, ensure_ascii=False)
            logger.info(f"Successfully saved update check results from {len(all_update_results)} sources.")
        except IOError as e:
            logger.error(f"Error saving update check results to {results_file}: {e}")
        except TypeError as e:
             logger.error(f"Error serializing data to JSON: {e}")

    except Exception as e:
        logger.error(f"An error occurred during update checking: {str(e)}", exc_info=True)

if __name__ == "__main__":
    asyncio.run(main()) 