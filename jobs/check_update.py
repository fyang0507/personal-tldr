"""
[GENERATED BY CURSOR]
This script retrieves the latest updates from various content sources
listed in subscriptions.toml, checks for new content, and caches update information.

This implements the first phase of the two-phase content retrieval approach:
1. Check for updates and cache minimal metadata (this script)
2. Retrieve full content details when needed (implemented in jobs/preprocess.py)
"""

import os
from dotenv import load_dotenv
from utils.logging_config import logger
from connectors.youtube import check_latest_updates as youtube_check_updates
from connectors.podcast import check_latest_updates as podcast_check_updates
from connectors.bilibili import check_latest_updates as bilibili_check_updates
from connectors.website.pipeline import check_latest_updates as website_check_updates
from connectors.website.pipeline import get_validated_website_config
import json
from datetime import datetime
import pathlib
import time
from utils.toml_loader import load_toml_file
from utils.connector_cache import ConnectorCache

def load_environment():
    """
    Load and validate all required environment variables.

    Returns:
        dict: Dictionary containing required environment variables for retrieval.

    Raises:
        ValueError: If any required environment variable is missing.
    """
    load_dotenv()

    required_env_vars = {
        'YOUTUBE_API_KEY': os.getenv('YOUTUBE_API_KEY')
        # Add other retrieval-specific keys if needed in the future
    }

    missing_vars = [var for var, value in required_env_vars.items() if not value]
    if missing_vars:
        raise ValueError(f"Missing required environment variables for retrieval: {', '.join(missing_vars)}")

    return required_env_vars


def process_youtube_channels(youtube_channels, env_vars):
    """Check for updates from YouTube channels and return results."""
    results = []
    youtube_api_key = env_vars.get('YOUTUBE_API_KEY')
    if not youtube_api_key:
        logger.error("YouTube API Key is missing. Skipping YouTube processing.")
        return results

    for channel_name in youtube_channels:
        logger.info(f"Checking for updates from YouTube channel: {channel_name}")

        # Check for updates and cache results
        update_info = youtube_check_updates(channel_name, youtube_api_key)
        if update_info:
            results.append(update_info)
        else:
            logger.warning(f"No updates found for YouTube channel: {channel_name}")

    return results


def process_podcasts(podcast_names):
    """Check for updates from podcasts and return results."""
    results = []
    for podcast_name in podcast_names:
        logger.info(f"Checking for updates from podcast: {podcast_name}")
        
        # Check for updates and cache results
        update_info = podcast_check_updates(podcast_name)
        if update_info:
            results.append(update_info)
        else:
            logger.warning(f"No updates found for podcast: {podcast_name}")

    return results


def process_bilibili(bilibili_users):
    """Check for updates from Bilibili users and return results."""
    results = []
    
    if not bilibili_users:
        logger.info("No Bilibili users found in subscriptions.")
        return results
        
    for user in bilibili_users:
        uid = user.get('uid')
        name = user.get('name')
        
        if not uid:
            logger.error(f"Missing uid for Bilibili user: {name}")
            continue
            
        logger.info(f"Checking for updates from Bilibili user: {name} (uid: {uid})")
        
        # Check for updates and cache results
        update_info = bilibili_check_updates(uid, name)
        if update_info:
            results.append(update_info)
        else:
            logger.warning(f"No updates found for Bilibili user: {name}")
            
        # Add a small delay to avoid rate limiting
        time.sleep(2)
            
    return results


def process_websites(websites):
    """Check for updates from websites and return results."""
    results = []
    
    for website_subscription in websites:
        channel = website_subscription.get('channel')
        source_url = website_subscription.get('source_url')
        
        if not channel or not source_url:
            logger.error(f"Missing channel or source_url in website subscription: {website_subscription}. Skipping.")
            continue
            
        logger.info(f"Processing website subscription: {channel} ({source_url})")
        
        try:
            scraper_params_for_site = get_validated_website_config(source_url=source_url, channel=channel)
            logger.info(f"Using validated scraper config for {channel}: {scraper_params_for_site}")

            update_info = website_check_updates(
                channel=channel,
                source_url=source_url,
                website_config=scraper_params_for_site 
            )
            
            if update_info:
                if 'source_url' not in update_info:
                    update_info['source_url'] = source_url
                results.append(update_info)
            else:
                logger.warning(f"No updates found for website: {channel}")

        except ValueError as e:
            # Errors from get_validated_website_config (e.g., config not found, incomplete) are caught here
            logger.error(f"Skipping website {channel} due to configuration error: {e}")
        except Exception as e:
            # Other unexpected errors during website_check_updates
            logger.error(f"Error during website_check_updates for {channel} ({source_url}): {e}. Skipping.")
            
    return results


def main():
    """Main function to check for updates from all sources and save results."""
    try:
        # Load environment variables
        env_vars = load_environment()

        # Remove all previous caches
        cache = ConnectorCache()
        cache_dir = cache.cache_dir
        
        # Clear all files in cache directory
        if cache_dir.exists():
            logger.info(f"Clearing all caches from: {cache_dir}")
            for cache_file in cache_dir.glob("*.json"):
                try:
                    cache_file.unlink()
                    logger.debug(f"Removed cache file: {cache_file.name}")
                except Exception as e:
                    logger.error(f"Error removing cache file {cache_file.name}: {e}")
        
        # Load subscriptions
        subscriptions = load_toml_file("subscriptions.toml")

        # Process all content sources to check for updates
        youtube_results = process_youtube_channels(subscriptions.get('youtube', []), env_vars)
        podcast_results = process_podcasts(subscriptions.get('podcast', []))
        bilibili_results = process_bilibili(subscriptions.get('bilibili', []))
        website_results = process_websites(subscriptions.get('website', []))

        # Combine all results
        all_update_results = youtube_results + podcast_results + bilibili_results + website_results

        # Convert any non-serializable objects to strings for JSON
        for result in all_update_results:
            for key, value in result.items():
                if isinstance(value, datetime):
                    result[key] = value.isoformat()

        if not all_update_results:
            logger.warning("No updates found from any source.")
            return

        # Save results
        data_dir = pathlib.Path("data")
        data_dir.mkdir(exist_ok=True)
        current_date_str = datetime.now().strftime("%Y-%m-%d")
        results_file = data_dir / f"update_check_results_{current_date_str}.json"

        logger.info(f"Saving update check results to: {results_file}")
        try:
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(all_update_results, f, indent=2, ensure_ascii=False)
            logger.info(f"Successfully saved update check results from {len(all_update_results)} sources.")
        except IOError as e:
            logger.error(f"Error saving update check results to {results_file}: {e}")
        except TypeError as e:
             logger.error(f"Error serializing data to JSON: {e}")

    except Exception as e:
        logger.error(f"An error occurred during update checking: {str(e)}")

if __name__ == "__main__":
    main() 