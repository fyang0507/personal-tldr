"""
[GENERATED BY CURSOR]
This script retrieves the latest content from YouTube channels and podcasts
listed in subscriptions.toml and saves the raw data to a JSON file.
"""

import os
import tomllib
from dotenv import load_dotenv
import sys
# Import the logger from the centralized logging_config module
from utils.logging_config import logger
from connectors.youtube import get_channel_id_from_name, get_latest_video_metadata
from connectors.podcast import get_latest_episode
from connectors.bilibili import get_user_videos, format_video_data
import json
from datetime import datetime
import pathlib
import email.utils  # For parsing RFC 822 date format
import time

def load_environment():
    """
    Load and validate all required environment variables.

    Returns:
        dict: Dictionary containing required environment variables for retrieval.

    Raises:
        ValueError: If any required environment variable is missing.
    """
    load_dotenv()

    required_env_vars = {
        'YOUTUBE_API_KEY': os.getenv('YOUTUBE_API_KEY')
        # Add other retrieval-specific keys if needed in the future
    }

    missing_vars = [var for var, value in required_env_vars.items() if not value]
    if missing_vars:
        raise ValueError(f"Missing required environment variables for retrieval: {', '.join(missing_vars)}")

    return required_env_vars

def load_subscriptions():
    """Load subscriptions from the TOML file."""
    try:
        with open("subscriptions.toml", "rb") as f:
            return tomllib.load(f)
    except FileNotFoundError:
        logger.error("subscriptions.toml not found.")
        raise
    except tomllib.TOMLDecodeError as e:
        logger.error(f"Error decoding subscriptions.toml: {e}")
        raise


def process_youtube_channels(youtube_channels, env_vars):
    """Process YouTube channels and return their latest video metadata."""
    results = []
    youtube_api_key = env_vars.get('YOUTUBE_API_KEY')
    if not youtube_api_key:
        logger.error("YouTube API Key is missing. Skipping YouTube processing.")
        return results

    for channel_name in youtube_channels:
        logger.info(f"Processing YouTube channel: {channel_name}")

        # Get channel ID
        channel_id = get_channel_id_from_name(channel_name, youtube_api_key)
        if not channel_id:
            logger.error(f"Could not find channel ID for '{channel_name}'")
            continue

        # Get latest video metadata
        metadata = get_latest_video_metadata(channel_id, youtube_api_key)
        if metadata:
            results.append({
                'type': 'youtube',
                'channel': channel_name,
                **metadata  # Directly inject metadata dictionary
            })

    return results

def process_podcasts(podcast_names):
    """Process podcasts and return their latest episode metadata."""
    results = []
    for podcast_name in podcast_names:
        logger.info(f"Processing podcast: {podcast_name}")

        # Get latest episode metadata
        metadata = get_latest_episode(podcast_name)
        if isinstance(metadata, dict):  # Check if we got valid metadata
            # Parse published_at date if it exists (typically in RFC 822 format)
            if 'published_at' in metadata and metadata['published_at']:
                try:
                    # Parse the RFC 822 date format
                    dt = email.utils.parsedate_to_datetime(metadata['published_at'])
                    # Set only the date part
                    metadata['published_at'] = dt.date().isoformat()
                except (ValueError, TypeError) as e:
                    logger.warning(f"Could not parse published_at date for podcast '{podcast_name}': {e}")
            
            results.append({
                'type': 'podcast',
                'channel': podcast_name,
                **metadata  # Directly inject metadata dictionary
            })
        else:
             logger.warning(f"Could not fetch valid metadata for podcast '{podcast_name}'. Received: {metadata}")

    return results

def process_bilibili(bilibili_users):
    """Process Bilibili users and return their latest video metadata."""
    results = []
    
    if not bilibili_users:
        logger.info("No Bilibili users found in subscriptions.")
        return results
        
    for user in bilibili_users:
        uid = user.get('uid')
        name = user.get('name')
        
        if not uid:
            logger.error(f"Missing uid for Bilibili user: {name}")
            continue
            
        logger.info(f"Processing Bilibili user: {name} (uid: {uid})")
        
        # Get latest video metadata (limit to 1)
        response = get_user_videos(uid, page_size=1)
        
        if response.get("code") != 0:
            logger.error(f"Error fetching videos for Bilibili user '{name}': {response.get('message', 'Unknown error')}")
            continue
            
        if "data" not in response or "archives" not in response["data"] or not response["data"]["archives"]:
            logger.warning(f"No videos found for Bilibili user '{name}'")
            continue
            
        # Get the latest video
        latest_video = response["data"]["archives"][0]
        time.sleep(5) # sleep for 5 seconds to avoid rate limit
        
        # Format the video data
        metadata = format_video_data(latest_video)
        
        if metadata:
            results.append({
                'type': 'bilibili',
                'channel': name,
                **metadata  # Directly inject metadata dictionary
            })

        else:
            logger.warning(f"Could not format metadata for Bilibili user '{name}'")
            
    return results

def main():
    """Main function to retrieve data and save it."""
    try:
        # Load environment variables
        env_vars = load_environment()

        # Load subscriptions
        subscriptions = load_subscriptions()

        # Process YouTube channels
        youtube_results = process_youtube_channels(subscriptions.get('youtube', []), env_vars)

        # Process podcasts
        podcast_results = process_podcasts(subscriptions.get('podcast', []))
        
        # Process Bilibili users
        bilibili_results = process_bilibili(subscriptions.get('bilibili', []))

        # Combine all results
        all_raw_results = youtube_results + podcast_results + bilibili_results

        # --- Convert datetime objects to strings for JSON serialization ---
        for result in all_raw_results:
            for key, value in result.items():
                if isinstance(value, datetime):
                    result[key] = value.date().isoformat()

        if not all_raw_results:
            logger.warning("No data retrieved from any source.")
            return

        # --- Save Raw Data ---
        data_dir = pathlib.Path("data")
        data_dir.mkdir(exist_ok=True) # Ensure data directory exists
        current_date_str = datetime.now().strftime("%Y-%m-%d")
        raw_data_file = data_dir / f"raw_results_{current_date_str}.json"

        logger.info(f"Saving raw results to: {raw_data_file}")
        try:
            with open(raw_data_file, 'w', encoding='utf-8') as f:
                json.dump(all_raw_results, f, indent=2, ensure_ascii=False)
            logger.info("Successfully saved raw data.")
        except IOError as e:
            logger.error(f"Error saving raw data to {raw_data_file}: {e}")
        except TypeError as e:
             logger.error(f"Error serializing data to JSON: {e}")


    except Exception as e:
        logger.error(f"An error occurred during data retrieval: {str(e)}")
        # Consider re-raising if this script is part of a larger workflow
        # raise

if __name__ == "__main__":
    main() 