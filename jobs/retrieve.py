"""
[GENERATED BY CURSOR]
This script retrieves the latest content from YouTube channels and podcasts
listed in subscriptions.toml and saves the raw data to a JSON file.
"""

import os
import tomllib
from dotenv import load_dotenv
from loguru import logger
from connectors.youtube import get_channel_id_from_name, get_latest_video_metadata
from connectors.podcast import get_latest_episode
import json
from datetime import datetime
import pathlib
import email.utils  # For parsing RFC 822 date format

def load_environment():
    """
    Load and validate all required environment variables.

    Returns:
        dict: Dictionary containing required environment variables for retrieval.

    Raises:
        ValueError: If any required environment variable is missing.
    """
    load_dotenv()

    required_env_vars = {
        'YOUTUBE_API_KEY': os.getenv('YOUTUBE_API_KEY')
        # Add other retrieval-specific keys if needed in the future
    }

    missing_vars = [var for var, value in required_env_vars.items() if not value]
    if missing_vars:
        raise ValueError(f"Missing required environment variables for retrieval: {', '.join(missing_vars)}")

    return required_env_vars

def load_subscriptions():
    """Load subscriptions from the TOML file."""
    try:
        with open("subscriptions.toml", "rb") as f:
            return tomllib.load(f)
    except FileNotFoundError:
        logger.error("subscriptions.toml not found.")
        raise
    except tomllib.TOMLDecodeError as e:
        logger.error(f"Error decoding subscriptions.toml: {e}")
        raise


def process_youtube_channels(youtube_channels, env_vars):
    """Process YouTube channels and return their latest video metadata."""
    results = []
    youtube_api_key = env_vars.get('YOUTUBE_API_KEY')
    if not youtube_api_key:
        logger.error("YouTube API Key is missing. Skipping YouTube processing.")
        return results

    for channel_name in youtube_channels:
        logger.info(f"Processing YouTube channel: {channel_name}")

        # Get channel ID
        channel_id = get_channel_id_from_name(channel_name, youtube_api_key)
        if not channel_id:
            logger.error(f"Could not find channel ID for '{channel_name}'")
            continue

        # Get latest video metadata
        metadata = get_latest_video_metadata(channel_id, youtube_api_key)
        if metadata:
            results.append({
                'type': 'youtube',
                'channel': channel_name,
                'data': metadata  # Keep raw metadata here
            })

    return results

def process_podcasts(podcast_names):
    """Process podcasts and return their latest episode metadata."""
    results = []
    for podcast_name in podcast_names:
        logger.info(f"Processing podcast: {podcast_name}")

        # Get latest episode metadata
        metadata = get_latest_episode(podcast_name)
        if isinstance(metadata, dict):  # Check if we got valid metadata
            # Parse published_at date if it exists (typically in RFC 822 format)
            if 'published_at' in metadata and metadata['published_at']:
                try:
                    # Parse the RFC 822 date format
                    dt = email.utils.parsedate_to_datetime(metadata['published_at'])
                    # Set only the date part
                    metadata['published_at'] = dt.date().isoformat()
                except (ValueError, TypeError) as e:
                    logger.warning(f"Could not parse published_at date for podcast '{podcast_name}': {e}")
            
            results.append({
                'type': 'podcast',
                'channel': podcast_name,
                'data': metadata  # Updated metadata with date-only published_at
            })
        else:
             logger.warning(f"Could not fetch valid metadata for podcast '{podcast_name}'. Received: {metadata}")

    return results

def main():
    """Main function to retrieve data and save it."""
    try:
        # Load environment variables
        env_vars = load_environment()

        # Load subscriptions
        subscriptions = load_subscriptions()

        # Process YouTube channels
        youtube_results = process_youtube_channels(subscriptions.get('youtube', []), env_vars)

        # Process podcasts
        podcast_results = process_podcasts(subscriptions.get('podcast', []))

        # Combine all results
        all_raw_results = youtube_results + podcast_results

        # --- Convert datetime objects to strings for JSON serialization ---
        for result in all_raw_results:
            if 'data' in result and isinstance(result['data'], dict):
                # Convert datetime objects to date strings (without time)
                for key, value in result['data'].items():
                    if isinstance(value, datetime):
                        result['data'][key] = value.date().isoformat()

        if not all_raw_results:
            logger.warning("No data retrieved from any source.")
            return

        # --- Save Raw Data ---
        data_dir = pathlib.Path("data")
        data_dir.mkdir(exist_ok=True) # Ensure data directory exists
        current_date_str = datetime.now().strftime("%Y-%m-%d")
        raw_data_file = data_dir / f"raw_results_{current_date_str}.json"

        logger.info(f"Saving raw results to: {raw_data_file}")
        try:
            with open(raw_data_file, 'w') as f:
                json.dump(all_raw_results, f, indent=2)
            logger.info("Successfully saved raw data.")
        except IOError as e:
            logger.error(f"Error saving raw data to {raw_data_file}: {e}")
        except TypeError as e:
             logger.error(f"Error serializing data to JSON: {e}")


    except Exception as e:
        logger.error(f"An error occurred during data retrieval: {str(e)}")
        # Consider re-raising if this script is part of a larger workflow
        # raise

if __name__ == "__main__":
    main() 