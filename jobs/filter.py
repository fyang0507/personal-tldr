"""
[GENERATED BY CURSOR]
This script processes and filters content updates from various sources.

Workflow:
1. Loads raw results from today's data collection
2. Deduplicates current entries to keep only the latest entry per channel
3. Checks for stale subscriptions that haven't been updated in 30+ days
4. Filters raw results against deduplicated entries to identify new content
5. Exports filtered results to a JSON file with today's date
6. Updates the current entries list with new content

When running in GitHub Actions, it reads from and writes to a GitHub Gist instead
of local files, allowing for persistent state between workflow runs.
"""

import json
from datetime import datetime
import os
# Import the logger from the centralized logging_config module
from utils.logging_config import logger
from dotenv import load_dotenv
from connectors.gist import read_from_gist, update_gist

def is_github_actions_env():
    return os.getenv("GITHUB_ACTIONS") == "true"

def deduplicate_current_entries(current_entries):
    """
    Deduplicate current entries to keep only the latest entry for each channel.
    Returns a new list with one entry per channel (the one with the latest published_at date).
    """
    # Dictionary to keep track of the latest entry for each channel
    latest_entries = {}
    
    # Iterate through all entries to find the latest for each channel
    for entry in current_entries:
        channel = entry['channel']
        published_at = entry['published_at']
        
        if channel not in latest_entries or published_at > latest_entries[channel]['published_at']:
            latest_entries[channel] = entry
    
    # Convert dictionary values back to a list
    deduplicated_entries = list(latest_entries.values())
    
    logger.info(f"Deduplicated from {len(current_entries)} to {len(deduplicated_entries)} entries")
    return deduplicated_entries

def check_stale_subscriptions(deduplicated_entries, days_threshold=30):
    """
    Check if any subscriptions haven't been updated for more than the specified number of days.
    Logs an error for each subscription that is older than the threshold.
    """
    today = datetime.now()
    stale_count = 0
    
    for entry in deduplicated_entries:
        channel = entry['channel']
        published_at = entry['published_at']
        
        # Convert the published_at string to a datetime object
        try:
            # Assuming published_at is in ISO format (YYYY-MM-DD)
            last_update_date = datetime.fromisoformat(published_at.split('T')[0] if 'T' in published_at else published_at)
            
            # Calculate the difference in days
            days_since_update = (today - last_update_date).days
            
            if days_since_update > days_threshold:
                logger.error(f"Subscription '{channel}' hasn't been updated for {days_since_update} days (last update: {published_at})")
                stale_count += 1
        except (ValueError, TypeError) as e:
            logger.error(f"Error parsing date for channel '{channel}': {e}")
    
    if stale_count > 0:
        logger.warning(f"Found {stale_count} stale subscriptions (older than {days_threshold} days)")
    else:
        logger.info(f"All subscriptions are up to date (updated within {days_threshold} days)")
    
    return stale_count

def filter_new_entries(raw_results, current_entries):
    """
    Filter raw results to keep only new entries that are not in current_entries.
    Returns a tuple of (filtered_results, updated_current_entries).
    """
    # Create a set of (channel, published_at) tuples from current entries for efficient lookup
    current_set = {(entry['channel'], entry['published_at']) for entry in current_entries}
    
    # Filter raw results to keep only new entries
    filtered_results = []
    for item in raw_results:
        channel = item['channel']
        published_at = item['published_at']
        
        # Check if this entry is not in current_entries
        if (channel, published_at) not in current_set:
            filtered_results.append(item)
            # Add the new entry to current_entries
            current_entries.append({
                'channel': channel,
                'published_at': published_at
            })
            logger.info(f"Adding {channel} on {published_at}")
        else:
            logger.info(f"Skipping {channel} on {published_at} because it already exists")
    
    return filtered_results, current_entries

def load_raw_results(today):
    """
    Load raw results from the daily JSON file.
    Returns the raw results data or None if the file doesn't exist.
    """
    raw_results_path = f"data/raw_results_{today}.json"
    
    # Check if raw results file exists
    if not os.path.exists(raw_results_path):
        logger.error(f"Error: {raw_results_path} does not exist.")
        return None
    
    # Load raw results
    with open(raw_results_path, 'r', encoding='utf-8') as f:
        raw_results = json.load(f)
    
    return raw_results

def save_filtered_results(filtered_results, today):
    """Save filtered results to a local file"""
    # Ensure the data directory exists
    os.makedirs("data", exist_ok=True)
    
    # Define path for filtered results
    filtered_results_path = f"data/filtered_results_{today}.json"
    
    # Write filtered results to a new file
    with open(filtered_results_path, 'w', encoding='utf-8') as f:
        json.dump(filtered_results, f, ensure_ascii=False, indent=2)
    
    logger.info(f"Results saved to {filtered_results_path}")

def main():
    # Get today's date in the format YYYY-MM-DD
    today = datetime.now().strftime("%Y-%m-%d")
    
    # Load environment variables
    load_dotenv()
    
    # Load raw results
    raw_results = load_raw_results(today)
    if raw_results is None:
        return
    
    # Check if running in GitHub Actions
    if is_github_actions_env():
        logger.info("Running in GitHub Actions environment")
        
        # Get Gist ID and GitHub token from environment variables
        gist_id = os.getenv("GIST_ID")
        gist_token = os.getenv("GIST_TOKEN")
        
        if not gist_id:
            logger.error("Missing GIST_ID in environment variables")
            return
        
        # Load current entries from Gist
        try:
            current_entries = read_from_gist(gist_id, gist_token, "current.json")
        except Exception as e:
            logger.error(f"Error reading from gist: {e}")
            return
        
        # Deduplicate current entries first
        deduplicated_entries = deduplicate_current_entries(current_entries)
        
        # Check for stale subscriptions
        check_stale_subscriptions(deduplicated_entries)
        
        # Filter raw results and get updated current entries
        filtered_results, updated_current_entries = filter_new_entries(raw_results, deduplicated_entries)
        
        # Save filtered results locally
        save_filtered_results(filtered_results, today)
        
        # Update both files in the gist
        try:
            files_data = {
                "current.json": {
                    "content": json.dumps(updated_current_entries, ensure_ascii=False, indent=2)
                },
            }
            update_gist(gist_id, gist_token, files_data)
            logger.info("Updated gist with current entries and filtered results")
        except Exception as e:
            logger.error(f"Error updating gist: {e}")
            return
        
        logger.info(f"Filtered {len(filtered_results)} new entries out of {len(raw_results)} total.")
    
    else:
        logger.info("Running in local development environment")
        
        # Define file path for current entries
        current_path = "data/current.json"
        
        # Check if current.json exists
        if not os.path.exists(current_path):
            logger.error(f"Error: {current_path} does not exist.")
            return
        
        # Load current entries
        with open(current_path, 'r', encoding='utf-8') as f:
            current_entries = json.load(f)
        
        # Deduplicate current entries first
        deduplicated_entries = deduplicate_current_entries(current_entries)
        
        # Check for stale subscriptions
        check_stale_subscriptions(deduplicated_entries)
        
        # Filter raw results and get updated current entries
        filtered_results, updated_current_entries = filter_new_entries(raw_results, deduplicated_entries)
        
        # Save filtered results locally
        save_filtered_results(filtered_results, today)

        # Update current.json with the new entries
        with open(current_path, 'w', encoding='utf-8') as f:
            json.dump(updated_current_entries, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Filtered {len(filtered_results)} new entries out of {len(raw_results)} total.")

if __name__ == "__main__":
    main()
