"""
[GENERATED BY CURSOR]
This script processes data using prompts from preprocess.toml.
It cleans and structures content details from various sources according to the specified format.
Key features:
- Extracts and normalizes metadata from different content sources
- Summarizes descriptions using LLM-based processing
- Formats duration, statistics, and publication dates consistently
- Supports multiple content types including YouTube videos and podcasts

Part of the two-phase content retrieval workflow:
1. retrieve.py - Checks for updates and caches metadata
2. filter.py - Identifies new content to retrieve
3. fetch_content.py - Retrieves full content details
4. preprocess.py (this script) - Processes and normalizes content
"""

import os
import json
import re
from datetime import datetime
# Import the logger from the centralized logging_config module
from utils.logging_config import logger
from connectors.llm import api_text_completion
from utils.toml_loader import load_toml_file
from utils.llm_response_format import parse_bullet_points


def json_datetime_serializer(obj):
    """JSON serializer for objects not serializable by default json code"""
    if isinstance(obj, datetime):
        return obj.isoformat()
    raise TypeError ("Type %s not serializable" % type(obj))


def load_prompt_config():
    """Load the prompt configuration from the TOML file."""
    config = load_toml_file("prompts/preprocess.toml")
    return config["description_summary"]["system"], config["description_summary"]["model"]


def process_title(raw_data):
    """Extract and clean the title from raw data."""
    return raw_data.get('title', '').strip()


def process_channel(raw_data):
    """Extract the channel name from raw data."""
    return raw_data.get('channel', '').strip()


def process_content_type(raw_data):
    """Determine content type from raw data."""
    return raw_data.get('type', 'unknown').strip()


def process_published_at(raw_data):
    """Process and format the published date from raw data."""
    published_at = None
    if 'published_at' in raw_data:
        published_at = raw_data['published_at']
    
    # Format to ISO date (only date part)
    if published_at:
        if isinstance(published_at, str):
            # Try to parse the string to a datetime object
            try:
                dt = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
                published_at = dt.date().isoformat()
            except (ValueError, TypeError):
                published_at = None
        elif isinstance(published_at, datetime):
            published_at = published_at.date().isoformat()
    
    return published_at


def process_url(raw_data):
    """Extract or construct the URL from raw data."""
    if 'url' in raw_data:
        url = raw_data.get('url', '') 
    elif 'link' in raw_data:
        url = raw_data.get('link', '')
    else:
        url = ''
    return url


def process_stats(raw_data):
    """Process statistics from raw data."""
    return raw_data.get('stats', {})


def process_duration(raw_data):
    """Process and format duration from raw data."""
    iso_duration = raw_data.get('duration', '')

    # Check if duration is already in human-readable format like "6m 2s" or "4 m 20 s"
    if isinstance(iso_duration, str) and re.match(r'^\s*\d+\s*[hm]\s*(?:\d+\s*[ms])?\s*(?:\d+\s*s)?\s*$', iso_duration):
        return iso_duration
        
    # if duration is numerical, convert to human-readable format in minutes and seconds
    if isinstance(iso_duration, (int, str)) and str(iso_duration).isdigit():
        iso_duration = int(iso_duration)
        minutes = iso_duration // 60
        seconds = iso_duration % 60
        return f"{minutes}m {seconds}s"

    # if duration is in the form of "hh:mm:ss" or "mm:ss", convert to human-readable format
    if ':' in iso_duration:
        parts = iso_duration.split(':')
        if len(parts) == 3:  # hh:mm:ss format
            hours, minutes, seconds = map(int, parts)
            return f"{hours}h {minutes}m {seconds}s"
        elif len(parts) == 2:  # mm:ss format
            minutes, seconds = map(int, parts)
            return f"{minutes}m {seconds}s"
    
    # Convert ISO 8601 duration to human-readable format
    if iso_duration.startswith('PT'):
        hours = 0
        minutes = 0
        seconds = 0
        
        # Remove PT prefix
        duration_str = iso_duration[2:]
        
        # Extract hours, minutes, seconds
        if 'H' in duration_str:
            hours_part = duration_str.split('H')[0]
            duration_str = duration_str.split('H')[1]
            hours = int(hours_part)
        
        if 'M' in duration_str:
            minutes_part = duration_str.split('M')[0]
            duration_str = duration_str.split('M')[1]
            minutes = int(minutes_part)
        
        if 'S' in duration_str:
            seconds_part = duration_str.split('S')[0]
            seconds = int(seconds_part)
        
        # Format duration string
        duration_parts = []
        if hours > 0:
            duration_parts.append(f"{hours}h")
        if minutes > 0 or hours > 0:
            duration_parts.append(f"{minutes}m")
        if seconds > 0 or not duration_parts:
            duration_parts.append(f"{seconds}s")
        
        duration = ' '.join(duration_parts)
    return duration


def process_summary_with_llm(raw_data):
    """Process the description into a summary using LLM."""
    # Load prompt configuration
    system_prompt, model = load_prompt_config()

    # Create a simplified version of the data containing only necessary fields for summary generation
    summary_data = {
        "title": process_title(raw_data),
        "description": raw_data.get('description', '')
    }

    if summary_data["description"] == "":
        return ""
    
    # Prepare the user message with the data for summary
    user_message = f"{json.dumps(summary_data, indent=2)}"
    # Call the completion API
    response = api_text_completion(model, system_prompt, user_message)
    # Parse the response into a list of bullet points
    return parse_bullet_points(response)


def process_data(raw_data):
    """
    Process data using a combination of direct Python processing and LLM.
    
    Args:
        raw_data (dict): Raw video metadata
        
    Returns:
        dict: Processed and cleaned video metadata
    """    
    try:
        # Process each field using dedicated functions
        processed_data = {
            "title": process_title(raw_data),
            "channel": process_channel(raw_data),
            "type": process_content_type(raw_data),
            "published_at": process_published_at(raw_data),
            "url": process_url(raw_data),
            "stats": process_stats(raw_data),
            "duration": process_duration(raw_data),
            "summary": process_summary_with_llm(raw_data)  # Use LLM for summary
        }
        
        return processed_data
    
    except Exception as e:
        logger.error(f"Error processing data: {e}")
        return None


def main():
    """Main function to process content data."""
    # Get today's date in the format YYYY-MM-DD
    today = datetime.now().strftime("%Y-%m-%d")
    
    # Define file paths
    raw_results_path = f"data/raw_results_{today}.json"
    processed_results_path = f"data/processed_results_{today}.json"
    
    # Check if raw results file exists
    if not os.path.exists(raw_results_path):
        logger.error(f"Error: {raw_results_path} does not exist.")
        return
    
    # Load raw results
    with open(raw_results_path, 'r', encoding='utf-8') as f:
        raw_results = json.load(f)
    
    processed_results = []
    
    # Process each item in the raw results
    for index, item in enumerate(raw_results, 1):
        # Log the item being processed with progress status
        logger.info(f"Processing item {index}/{len(raw_results)}: {item.get('channel')} - {item.get('type')}")
        
        processed_data = process_data(item)
        if processed_data:
            processed_results.append(processed_data)
    
    # Save processed results
    with open(processed_results_path, 'w', encoding='utf-8') as f:
        json.dump(processed_results, f, ensure_ascii=False, indent=2)
    
    logger.info(f"Processed {len(processed_results)} content items.")
    logger.info(f"Results saved to {processed_results_path}")


if __name__ == "__main__":
    main()
